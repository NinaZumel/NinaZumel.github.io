<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:base="en">
	<title>Nina B. Zumel</title>
	<subtitle>Data Scientist, San Francisco</subtitle>
	<link href="https://ninazumel.com/feed.xml" rel="self"/>
	<link href="https://ninazumel.com/"/>
	<updated>2024-09-04T00:00:00Z</updated>
	<id>https://ninazumel.com/</id>
	<author>
		<name>Nina B. Zumel</name>
		<email>nzumel@win-vector.com</email>
	</author>
	
	<entry>
		<title>Playing With Sidenotes</title>
		<link href="https://ninazumel.com/blog/2024-09-04-sidenotes/"/>
		<updated>2024-09-04T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2024-09-04-sidenotes/</id>
		<content type="html">&lt;p&gt;For no particular reason, I started wondering if I could add sidenotes to my blog. I&#39;ve always liked them better than endnotes, especially for longer documents. This led me to the more general question: &lt;em&gt;Is there an eleventy template for sidenotes&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;The answer is &lt;a href=&quot;https://eleventufte.netlify.app/&quot;&gt;yes&lt;/a&gt;. But before plunging in, I decided to try a fresh blog first. My &lt;a href=&quot;https://github.com/NinaZumel/TwelveMedievalGhostStories&quot;&gt;repository of medieval ghost stories&lt;/a&gt; (forked from an earlier repo) seemed like a useful test case.&lt;/p&gt;
&lt;p&gt;It was also a hard test case: these are scholarly documents, and the notes are long and dense -- exactly the case that Tufte-CSS based sidenote solutions don&#39;t handle well. I ended up using a &lt;a href=&quot;https://markllobrera.com/posts/sidenotes/&quot;&gt;javascript based solution&lt;/a&gt; that I found on the web.&lt;/p&gt;
&lt;p&gt;The result is the &lt;a href=&quot;https://ninazumel.com/TwelveMedievalGhostStories/&quot;&gt;Twelve Medieval Ghost Stories&lt;/a&gt; website.&lt;/p&gt;
&lt;p&gt;It looks great; the sidenotes work really well, and drop down to regular endnotes on narrow browsers. But the grid-based paradigm this solution uses seems really brittle, and I don&#39;t want to restructure my entire blog template to support it.&lt;/p&gt;
&lt;p&gt;So back to thinking about Tufte-CSS. But this wasn&#39;t a waste; I&#39;ve learned some potentially useful things, and this is a fairly nice presentation of the ghost stories. I&#39;ll just let it percolate in my head for a little longer.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Adjusting Saturated Multivariate Linear Models</title>
		<link href="https://ninazumel.com/blog/2024-08-20-saturated-models/"/>
		<updated>2024-08-20T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2024-08-20-saturated-models/</id>
		<content type="html">&lt;p&gt;This is a followup to the Win Vector blog article &lt;a href=&quot;https://win-vector.com/2024/08/16/post-hoc-adjustment-for-zero-thresholded-linear-models/&quot;&gt;&lt;em&gt;Post-hoc Adjustment for
Zero-Thresholded Linear
Models&lt;/em&gt;&lt;/a&gt;,
in which I showed how to use a one-variable GAM (e.g., a spline) to
adjust a linear model in a problem space where outcomes are strictly
nonnegative. If you haven’t read that article, I suggest you check it
out, first.&lt;/p&gt;
&lt;h2 id=&quot;some-background&quot; tabindex=&quot;-1&quot;&gt;Some Background &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-08-20-saturated-models/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The original motivation for the work we did here was to help a client
who had built a fairly complex multivariate model to predict expected
count data. Their underlying assumption was that in their domain, a
count of zero is a rare event. To quote my previous article:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When you don’t expect to see too many zeros in practice, modeling the
process as linear and thresholding negative predictions at zero is not
unreasonable. But the more zeros (saturations) you expect to see, the
less well a linear model will perform.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;They then automated the model training and deployment process, and put
it into production across multiple sites.&lt;/p&gt;
&lt;p&gt;Unfortunately, zero-counts are not so rare as they originally believed,
at some of their sites. Because coming up with and deploying a new model
design is not necessarily feasible at this stage (at least not in the
immediate term), they wanted to figure out how to adjust their existing
models to operate even at high zero-count sites. Note that this
adjustment is a &lt;em&gt;one-dimensional&lt;/em&gt; process: mapping the output of one
model to a prediction that is closer to the actual outcomes.&lt;/p&gt;
&lt;p&gt;To motivate this adjustment process in the previous article, I used an
example of a linear model that was fit to a one-dimensional saturated
process. This was so I could plot the resulting “hockey stick” function,
and show what happens with different model adjustments. But it’s been my
experience that people won’t believe that this procedure will generalize
to multivariate linear models, unless I show them that it works on such
models. So in this article, I’ll apply the same procedures to a linear
model that was fit to two variables, just to prove the point.&lt;/p&gt;
&lt;p&gt;I’ll also add an additional adjustment that wasn’t in the original
article, but was suggested by Neal Fultz: adjustment using a Tobit
model. You can see more details of that in a revised version of the
article on github,
&lt;a href=&quot;https://github.com/WinVector/Examples/blob/main/linear_regression_w_zeros/lm_adjust_wtobit.md&quot;&gt;here&lt;/a&gt;.
Tobit adjustments work nearly as well as GAM adjustments, and do have
the (potential) advantage of having a stronger inductive bias, if you
believe that your process is truly linear in its non-saturated regions
&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;https://ninazumel.com/blog/2024-08-20-saturated-models/&quot; id=&quot;fnref1&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;In the second part of the article, I’ll try fitting a model directly to
the input data, using both GAM and Tobit, since ideally, that’s what
you’d do if you were approaching this modeling task &lt;em&gt;de novo&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For legibility and brevity, I’m going to hide a lot of the code when
generating this article, but you can find the original R Markdown source
code on github as well,
&lt;a href=&quot;https://github.com/WinVector/Examples/blob/main/linear_regression_w_zeros/lm2d_adjust.Rmd&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;example-problem&quot; tabindex=&quot;-1&quot;&gt;Example Problem &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-08-20-saturated-models/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Here are the characteristics of our example problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The feature &lt;code&gt;u&lt;/code&gt; positively correlates with outcome, while &lt;code&gt;v&lt;/code&gt;
negatively correlates.&lt;/li&gt;
&lt;li&gt;The outcome &lt;code&gt;y&lt;/code&gt; is constrained to be nonnegative; in other words, it
“saturates” at zero.&lt;/li&gt;
&lt;li&gt;Both features &lt;code&gt;u&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt; are bounded between 0 and 1 uniformly.&lt;/li&gt;
&lt;li&gt;There is a moderate noise process on top of it.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;trueprocess &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;N&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  u &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; runif&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;N&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  v &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; runif&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;N&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  noise &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.3&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; rnorm&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;N&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; 
  y &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; u &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; v &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; noise&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  data.frame&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;u &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; u&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; v &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; v&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; y &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; y&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;the-data&quot; tabindex=&quot;-1&quot;&gt;The Data &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-08-20-saturated-models/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Let’s generate some training data.&lt;/p&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;traind &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; trueprocess&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
head&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-text&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;    ##           u         v         y
    ## 1 0.1634115 0.2125462 0.6570359
    ## 2 0.5249624 0.6152054 0.0000000
    ## 3 0.3051476 0.9153664 0.0000000
    ## 4 0.9277528 0.8064937 1.5395565
    ## 5 0.5715193 0.9990540 0.0000000
    ## 6 0.2602110 0.5206304 0.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s plot a heatmap of the training data. Note that there’s lots of
saturation.&lt;/p&gt;
&lt;img alt=&quot;Heatmap of the true process&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/unnamed-chunk-6-1-Urzcld1ZwK-400.png&quot; width=&quot;672&quot; height=&quot;480&quot; srcset=&quot;https://ninazumel.com/img/unnamed-chunk-6-1-Urzcld1ZwK-400.png 400w, https://ninazumel.com/img/unnamed-chunk-6-1-Urzcld1ZwK-672.png 672w&quot; sizes=&quot;100vw&quot;&gt;
&lt;!--  ![](lm2d_adjust_files/figure-gfm/unnamed-chunk-6-1.png)--&gt;
&lt;p&gt;We’re going to use all the data to train our models, but it’s not easy
to show comparison plots in three dimensions in a way that’s legible. So
we’ll also look at slices of the data, by holding &lt;code&gt;u&lt;/code&gt; restricted to a
narrow range around a nominal value. Here’s a slice with &lt;code&gt;u&lt;/code&gt; fixed to
near 0.5.&lt;/p&gt;
&lt;img alt=&quot;Slice of training data around u=0.5&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/unnamed-chunk-7-1-8SjDrKYvhC-400.png&quot; width=&quot;672&quot; height=&quot;480&quot; srcset=&quot;https://ninazumel.com/img/unnamed-chunk-7-1-8SjDrKYvhC-400.png 400w, https://ninazumel.com/img/unnamed-chunk-7-1-8SjDrKYvhC-672.png 672w&quot; sizes=&quot;100vw&quot;&gt;
&lt;!-- ![](lm2d_adjust_files/figure-gfm/unnamed-chunk-7-1.png) --&gt;
&lt;p&gt;As you can see, the data is fairly noisy, in addition to being quite
saturated. Now let’s train a linear model on the data, as well as all
our adjusted models.&lt;/p&gt;
&lt;h2 id=&quot;part-i-adjustments-to-a-linear-model&quot; tabindex=&quot;-1&quot;&gt;Part I: Adjustments to a Linear Model &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-08-20-saturated-models/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&quot;train-the-initial-model-and-all-the-adjustments&quot; tabindex=&quot;-1&quot;&gt;Train the initial model, and all the adjustments &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-08-20-saturated-models/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We’ll fit the model, make the naive adjustment of thresholding the data
at zero, then fit all the candidate adjustment models. Note that only
the initial model ever makes reference to the input variables.&lt;/p&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# train the initial model, &lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# get the initial predictions and thresholded predictions&lt;/span&gt;

initial_model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; lm&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;y &lt;span class=&quot;token operator&quot;&gt;~&lt;/span&gt; u &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; v&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; data&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
traind&lt;span class=&quot;token operator&quot;&gt;$&lt;/span&gt;y_initial&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;initial_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; newdata&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
traind&lt;span class=&quot;token operator&quot;&gt;$&lt;/span&gt;y_pred0 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; traind&lt;span class=&quot;token operator&quot;&gt;$&lt;/span&gt;y_initial&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# fit the adjustment models&lt;/span&gt;
&lt;span class=&quot;token comment&quot;&gt;# (function definitions in R markdown document)&lt;/span&gt;
scaling_model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; fit_scaler&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;initial_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;y&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
linadj_model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; fit_linmod&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;initial_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;y&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
gamadj_model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; fit_gammod&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;initial_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;y&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
tobitadj_model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; fit_tobit&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;initial_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;y&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

adjustments &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; list&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;scaling_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; linadj_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; gamadj_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; tobitadj_model&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
names&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;adjustments&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; c&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;y_linscale&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;y_linadj&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;y_gamadj&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;y_tobitadj&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# make the predictions&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;adj &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; names&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;adjustments&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  traind&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;adj&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; do_predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;adjustments&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;adj&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; newdata&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at the RMSE and bias of each model.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;prediction_type&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;description&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;RMSE&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;bias&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;initial&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;initial model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.4438901&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;pred0&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;zero-thresholded model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.3721101&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.0842275&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;linscale&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;scale-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.3692881&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.1196263&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;linadj&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;linear-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2453422&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.0298617&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;gamadj&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;GAM-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2261695&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.0007868&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;tobitadj&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;Tobit-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2293762&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;-0.0097394&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Model RMSE and bias on training data&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As expected (if you’ve read the previous article), the GAM-adjusted
model has the lowest training RMSE, and also lower bias than the other
adjusted models. The Tobit-adjusted model has comparable RMSE, and
slightly more bias.&lt;/p&gt;
&lt;p&gt;We can try to visualize what’s happening, by holding &lt;code&gt;u&lt;/code&gt; constant at
different values and looking at slices of the prediction surfaces. We’ll
just compare the original linear model, GAM-adjustment, and
Tobit-adjustment.&lt;/p&gt;
&lt;img alt=&quot;A comparison of model adjustments across various slices of the prediction surface&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/unnamed-chunk-10-1-epZZgVNig--400.png&quot; width=&quot;768&quot; height=&quot;672&quot; srcset=&quot;https://ninazumel.com/img/unnamed-chunk-10-1-epZZgVNig--400.png 400w, https://ninazumel.com/img/unnamed-chunk-10-1-epZZgVNig--768.png 768w&quot; sizes=&quot;100vw&quot;&gt;
&lt;!-- ![](lm2d_adjust_files/figure-gfm/unnamed-chunk-10-1.png) --&gt;
&lt;h3 id=&quot;test-on-holdout&quot; tabindex=&quot;-1&quot;&gt;Test on holdout &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-08-20-saturated-models/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Let’s compare the models on holdout data. For this example the holdout
performances are similar to training.&lt;/p&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;testd &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; trueprocess&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

testd&lt;span class=&quot;token operator&quot;&gt;$&lt;/span&gt;y_initial&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;initial_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; newdata&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;testd&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
testd&lt;span class=&quot;token operator&quot;&gt;$&lt;/span&gt;y_pred0 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; testd&lt;span class=&quot;token operator&quot;&gt;$&lt;/span&gt;y_initial&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;adj &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; names&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;adjustments&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  testd&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;adj&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; do_predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;adjustments&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;adj&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; newdata&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;testd&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;prediction_type&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;description&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;RMSE&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;bias&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;initial&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;initial model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.4504353&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;-0.0046703&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;pred0&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;zero-thresholded model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.3750385&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.0836604&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;linscale&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;scale-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.3721833&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.1184737&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;linadj&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;linear-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2482977&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.0310791&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;gamadj&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;GAM-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2285600&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.0043918&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;tobitadj&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;Tobit-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2300590&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;-0.0064108&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Model RMSE and bias on holdout data&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;part-ii-fitting-directly-to-the-input-data&quot; tabindex=&quot;-1&quot;&gt;Part II : Fitting directly to the input data &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-08-20-saturated-models/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Now let’s try both Tobit and GAM directly on the input data. There are,
of course, other methods we could try, like Poisson or Zero-inflated
Poisson regression. But since we’ve been focusing on Tobit and GAM as
adjustments, we’ll just stick to those.&lt;/p&gt;
&lt;p&gt;Note that since neither of these models are restricted to nonnegative
predictions, we still have to threshold to zero.&lt;/p&gt;
&lt;p&gt;Let’s fit to our training data.&lt;/p&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;gam_model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; gam&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;y &lt;span class=&quot;token operator&quot;&gt;~&lt;/span&gt; s&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;u&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; s&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;v&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; data&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
traind&lt;span class=&quot;token operator&quot;&gt;$&lt;/span&gt;y_gam &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;gam_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; newdata&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

tobit_model &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; vglm&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;y &lt;span class=&quot;token operator&quot;&gt;~&lt;/span&gt; u &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; v&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; tobit&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;Lower&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; data&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
traind&lt;span class=&quot;token operator&quot;&gt;$&lt;/span&gt;y_tobit &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;tobit_model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; traind&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;mu&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll compare the full model fits to the GAM-adjusted and Tobit-adjusted
linear models.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;prediction_type&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;description&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;RMSE&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;bias&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;gamadj&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;GAM-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2261695&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.0007868&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;tobitadj&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;Tobit-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2293762&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;-0.0097394&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;gam&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;Full GAM model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.3505055&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.0668857&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;tobit&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;Full Tobit model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2280450&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;-0.0099883&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Model RMSE and bias on training data&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The full (thresholded) Tobit model does essentially as well as the
Tobit-adjusted linear model, but the thresholded GAM doesn’t do so well.
Since this problem is truly linear, the stronger inductive bias of the
Tobit model serves us well.&lt;/p&gt;
&lt;h3 id=&quot;test-on-holdout-1&quot; tabindex=&quot;-1&quot;&gt;Test on holdout &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-08-20-saturated-models/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We can also evaluate these models on holdout data.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;prediction_type&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;description&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;RMSE&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;bias&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;gamadj&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;GAM-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2285600&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.0043918&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;tobitadj&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;Tobit-adjusted model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2300590&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;-0.0064108&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;gam&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;Full GAM model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.3574985&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.0658623&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;tobit&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;Full Tobit model&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;0.2280260&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;-0.0072236&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Model RMSE and bias on holdout data&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We get similar results. We can plot some slices to get an idea of what’s
happening.&lt;/p&gt;
&lt;img alt=&quot;Comparing Linear to thresholded GAM and Tobit models across various slices of the data&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/unnamed-chunk-16-1-pL2DBI2WxK-400.png&quot; width=&quot;768&quot; height=&quot;672&quot; srcset=&quot;https://ninazumel.com/img/unnamed-chunk-16-1-pL2DBI2WxK-400.png 400w, https://ninazumel.com/img/unnamed-chunk-16-1-pL2DBI2WxK-768.png 768w&quot; sizes=&quot;100vw&quot;&gt;
&lt;!-- ![](lm2d_adjust_files/figure-gfm/unnamed-chunk-16-1.png) --&gt;
&lt;h2 id=&quot;conclusions&quot; tabindex=&quot;-1&quot;&gt;Conclusions &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-08-20-saturated-models/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For our original problem—post-hoc adjustments of an already established
modeling procedure—a GAM adjustment seems to be the best way to adapt
our modeling to higher-than-anticipated saturation frequency. However,
Tobit adjustment is competitive. If you have the opportunity to design
the modeling procedure &lt;em&gt;de novo&lt;/em&gt;, GAM may not be the best option; though
I admit, I didn’t spend any time trying to tune the model. If the
process you are modeling is well-approximated as linear in the
non-saturated region, then a thresholded Tobit model appears to be a
good choice.&lt;/p&gt;
&lt;p&gt;Obviously, to fit a full model, one can try many
more methods: Poisson regression, trees, MARS, boosting, random forest,
and so on. A typical task for the data scientist is to try many
plausible methods on the client’s data and pick the one that appears to
be the best practical trade-off for the given client.&lt;/p&gt;
&lt;hr class=&quot;footnotes-sep&quot;&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn1&quot; class=&quot;footnote-item&quot;&gt;&lt;p&gt;Statisticians generally use the word “censored” when talking about
processes that threshold out to a minimum or maximum value (or
both). Often, the assumption is that the measurements are censored
because of limitations of the measurement itself: for example, you
can’t track a subject for more than five years, so the longest
lifetime you can expect to measure is 5, even if the subject lives
for decades longer. Models like Tobit try to predict &lt;em&gt;as if&lt;/em&gt; the
data isn’t censored: they can predict a lifetime longer than 5
years, even though those lifetimes can’t be measured.&lt;/p&gt;
&lt;p&gt;I am using the word “saturated” to indicate that the outcome being measured
literally cannot go beyond the threshold: counts can never be
negative. A possibly useful analogy is the image from a digital
camera: too much light “blows out” the photo. If you are trying to
predict the intensity of the light that hit the camera sensor using
the image pixels as the outcome data, then the data is censored. If
you are trying to predict &lt;em&gt;the values of the pixels&lt;/em&gt;, then the data
is saturated. &lt;a href=&quot;https://ninazumel.com/blog/2024-08-20-saturated-models/&quot; class=&quot;footnote-backref&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
	</entry>
	
	<entry>
		<title>Simpson’s Paradox as a Logistic Regression</title>
		<link href="https://ninazumel.com/blog/2024-07-16-simpsons-paradox-as-logistic-regression/"/>
		<updated>2024-07-16T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2024-07-16-simpsons-paradox-as-logistic-regression/</id>
		<content type="html">&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Simpson&#39;s_paradox&quot;&gt;Simpson’s paradox&lt;/a&gt; is
when a trend that is present in various groups of data seems to
disappear or even reverse when those groups are combined. One sees
examples of this often in things like medical trials, and the phenomenon
is generally due to one or more unmodelled confounding variables.&lt;/p&gt;
&lt;p&gt;In the course of something else that I was working on, I was trying to
come with an example of a logistic regression analysis, where one of the
coefficients had a clearly incorrect sign. There are several reasons why
this might happen: separation or quasiseparation of the data being the
obvious ones. But Simpson’s paradox is another cause.&lt;/p&gt;
&lt;p&gt;I ended up not needing the example, but since I had it, I thought I’d
write it up, since I’ve never seen Simpson’s paradox presented in quite
this way before.&lt;/p&gt;
&lt;h2 id=&quot;synthetic-example-weight-loss-trial&quot; tabindex=&quot;-1&quot;&gt;Synthetic Example: Weight Loss Trial &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-07-16-simpsons-paradox-as-logistic-regression/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is a problem statement where we would expect the coefficients of a
logistic regression to be non-negative (except the intercept).&lt;/p&gt;
&lt;p&gt;Consider a trial that tests the efficacy of a specific eating regimen
(let’s say 16/8 intermittent fasting, which we’ll call &lt;code&gt;ifasting&lt;/code&gt;) and a
specific exercise regimen (a brisk 30 minute walk every day, which we’ll
just call &lt;code&gt;exercise&lt;/code&gt;). The goal (“success”) is to lose at least five
pounds by the end of the trial period. We’ve set up three treatment
groups, as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;200 subjects try exercise alone&lt;/li&gt;
&lt;li&gt;300 subjects try ifasting alone&lt;/li&gt;
&lt;li&gt;300 subjects try ifasting plus exercise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Prior to the trial, all the subjects led fairly sedentary lifestyles,
and weren’t dieting in any formal way.&lt;/p&gt;
&lt;p&gt;For these subjects, one might reasonably expect that neither exercise
nor ifasting would be &lt;em&gt;less&lt;/em&gt; successful for losing weight than doing
nothing. One would also reasonably expect that ifasting plus exercise
should do no worse than doing either one alone. Therefore, modeling the
results of such an experiment as a logistic regression should lead to a
model where the coefficients &lt;mjx-container class=&quot;MathJax&quot; jax=&quot;SVG&quot; style=&quot;direction: ltr; position: relative;&quot;&gt;&lt;svg style=&quot;overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.667ex;&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;7.349ex&quot; height=&quot;2.262ex&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewBox=&quot;0 -705 3248.3 1000&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;scale(1,-1)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;msub&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;1D6FD&quot; d=&quot;M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;TeXAtom&quot; transform=&quot;translate(599,-150) scale(0.707)&quot; data-mjx-texclass=&quot;ORD&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;1D456&quot; d=&quot;M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(345,0)&quot;&gt;&lt;path data-c=&quot;1D453&quot; d=&quot;M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(895,0)&quot;&gt;&lt;path data-c=&quot;1D44E&quot; d=&quot;M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(1424,0)&quot;&gt;&lt;path data-c=&quot;1D460&quot; d=&quot;M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(1893,0)&quot;&gt;&lt;path data-c=&quot;1D461&quot; d=&quot;M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(2254,0)&quot;&gt;&lt;path data-c=&quot;1D456&quot; d=&quot;M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(2599,0)&quot;&gt;&lt;path data-c=&quot;1D45B&quot; d=&quot;M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(3199,0)&quot;&gt;&lt;path data-c=&quot;1D454&quot; d=&quot;M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;mjx-assistive-mml unselectable=&quot;on&quot; display=&quot;inline&quot; style=&quot;top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mrow data-mjx-texclass=&quot;ORD&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;&lt;/mjx-assistive-mml&gt;&lt;/mjx-container&gt; and &lt;mjx-container class=&quot;MathJax&quot; jax=&quot;SVG&quot; style=&quot;direction: ltr; position: relative;&quot;&gt;&lt;svg style=&quot;overflow: visible; min-height: 1px; min-width: 1px; vertical-align: -0.439ex;&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;7.336ex&quot; height=&quot;2.034ex&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewBox=&quot;0 -705 3242.7 899&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;scale(1,-1)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;msub&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;1D6FD&quot; d=&quot;M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;TeXAtom&quot; transform=&quot;translate(599,-150) scale(0.707)&quot; data-mjx-texclass=&quot;ORD&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;1D452&quot; d=&quot;M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(466,0)&quot;&gt;&lt;path data-c=&quot;1D465&quot; d=&quot;M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(1038,0)&quot;&gt;&lt;path data-c=&quot;1D452&quot; d=&quot;M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(1504,0)&quot;&gt;&lt;path data-c=&quot;1D45F&quot; d=&quot;M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(1955,0)&quot;&gt;&lt;path data-c=&quot;1D450&quot; d=&quot;M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(2388,0)&quot;&gt;&lt;path data-c=&quot;1D456&quot; d=&quot;M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(2733,0)&quot;&gt;&lt;path data-c=&quot;1D460&quot; d=&quot;M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(3202,0)&quot;&gt;&lt;path data-c=&quot;1D452&quot; d=&quot;M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z&quot; style=&quot;stroke-width: 3;&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;mjx-assistive-mml unselectable=&quot;on&quot; display=&quot;inline&quot; style=&quot;top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; position: absolute; padding: 1px 0px 0px 0px; border: 0px; display: block; width: auto; overflow: hidden;&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mrow data-mjx-texclass=&quot;ORD&quot;&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;&lt;/mjx-assistive-mml&gt;&lt;/mjx-container&gt;
are both non-negative, as any treatment should increase the odds that
the subject loses weight.&lt;/p&gt;
&lt;p&gt;Let&#39;s show an example where our expectations aren&#39;t met, in R.&lt;/p&gt;
&lt;h3 id=&quot;trends-for-individual-subpopulations&quot; tabindex=&quot;-1&quot;&gt;Trends for individual subpopulations &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-07-16-simpsons-paradox-as-logistic-regression/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Suppose that, unbeknownst to the researchers, there are two
subpopulations amongst the subject. The first, Population A, responds
quite well to the intermittent fasting regimen.&lt;/p&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;generate_samples &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; exercise&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; total&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; successes&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; label&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  failures &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; total&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;successes
  data.frame&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; ifasting&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
             exercise &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; exercise&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
             success &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; c&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;rep&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; successes&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; rep&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; failures&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
             label&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;label&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# population A&lt;/span&gt;
popA &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; data.table&lt;span class=&quot;token operator&quot;&gt;::&lt;/span&gt;rbindlist&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;list&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
  generate_samples&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; exercise&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; total&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; successes&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;A&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
  generate_samples&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; exercise&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; total&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; successes&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;160&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;A&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
  generate_samples&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; exercise&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; total&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; successes&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;A&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# model just on population A&lt;/span&gt;
mA &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; glm&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;success &lt;span class=&quot;token operator&quot;&gt;~&lt;/span&gt; ifasting &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; exercise&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; data&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;popA&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; family&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;binomial&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
summary&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;mA&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    ## 
    ## Call:
    ## glm(formula = success ~ ifasting + exercise, family = binomial, 
    ##     data = popA)
    ## 
    ## Coefficients:
    ##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
    ## (Intercept)  -4.7028     0.8078  -5.822 5.82e-09 ***
    ## ifasting      6.0890     0.7882   7.725 1.12e-14 ***
    ## exercise      0.8109     0.3773   2.149   0.0316 *  
    ## ---
    ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 527.16  on 399  degrees of freedom
    ## Residual deviance: 284.79  on 397  degrees of freedom
    ## AIC: 290.79
    ## 
    ## Number of Fisher Scoring iterations: 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, the coefficients for &lt;code&gt;ifasting&lt;/code&gt; and &lt;code&gt;exercise&lt;/code&gt; are both
positive. We can look at the rates that the model predicts.&lt;/p&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# small frame to represent all possible states&lt;/span&gt;
testdata &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; data.frame&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
  ifasting &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; c&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
  exercise &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; c&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# get the success rates for each state&lt;/span&gt;
testdata&lt;span class=&quot;token operator&quot;&gt;$&lt;/span&gt;predictA &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;mA&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; newdata&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;testdata&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; type&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;response&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
testdata&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    ##   ifasting exercise    predictA
    ## 1        0        0 0.008988764
    ## 2        0        1 0.020000000
    ## 3        1        0 0.800000000
    ## 4        1        1 0.900000000
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# confirm it by doing the averages by hand&lt;/span&gt;
library&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;poorman&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; warn.conflicts &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# for data-wrangling; uses dplyr API&lt;/span&gt;

popA &lt;span class=&quot;token operator&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt; 
  mutate&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;gp &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; ifelse&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting &lt;span class=&quot;token operator&quot;&gt;&amp;amp;&lt;/span&gt; exercise&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&#39;both&#39;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                     ifelse&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&#39;ifast alone&#39;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&#39;exercise alone&#39;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt;
  group_by&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;gp&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt;
  summarize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;success_rate &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; mean&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;success&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    ##               gp success_rate
    ## 1           both         0.90
    ## 2 exercise alone         0.02
    ## 3    ifast alone         0.80
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For Population A, exercise alone has a 2% success rate, intermitting
fasting an 80% success rate, and both together have a 90% success rate.&lt;/p&gt;
&lt;p&gt;There is also another population, Population B, that has what you might
call a “stickier” metabolism. For them, intermittent fasting is not
quite as effective.&lt;/p&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;popB &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; data.table&lt;span class=&quot;token operator&quot;&gt;::&lt;/span&gt;rbindlist&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;list&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
  generate_samples&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; exercise&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; total&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; successes&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;B&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
  generate_samples&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; exercise&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; total&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; successes&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;B&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
  generate_samples&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; exercise&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; total&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; successes&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;B&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# model just on population B&lt;/span&gt;
mB &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; glm&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;success &lt;span class=&quot;token operator&quot;&gt;~&lt;/span&gt; ifasting &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; exercise&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; data&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;popB&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; family&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;binomial&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
summary&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;mB&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    ## 
    ## Call:
    ## glm(formula = success ~ ifasting + exercise, family = binomial, 
    ##     data = popB)
    ## 
    ## Coefficients:
    ##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
    ## (Intercept)  -5.0006     1.0353  -4.830 1.36e-06 ***
    ## ifasting      4.5951     1.0149   4.528 5.97e-06 ***
    ## exercise      0.4055     0.2483   1.633    0.103    
    ## ---
    ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 519.18  on 399  degrees of freedom
    ## Residual deviance: 423.06  on 397  degrees of freedom
    ## AIC: 429.06
    ## 
    ## Number of Fisher Scoring iterations: 7
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;testdata&lt;span class=&quot;token operator&quot;&gt;$&lt;/span&gt;predictB &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;mB&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; newdata&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;testdata&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; type&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;response&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
testdata&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    ##   ifasting exercise    predictA    predictB
    ## 1        0        0 0.008988764 0.006688963
    ## 2        0        1 0.020000000 0.010000000
    ## 3        1        0 0.800000000 0.400000000
    ## 4        1        1 0.900000000 0.500000000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For Population B, the coefficients for &lt;code&gt;ifasting&lt;/code&gt; and &lt;code&gt;exercise&lt;/code&gt; are
still positive, but smaller, and the success rates are lower.&lt;/p&gt;
&lt;h3 id=&quot;trends-for-the-whole-population&quot; tabindex=&quot;-1&quot;&gt;Trends for the whole population &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-07-16-simpsons-paradox-as-logistic-regression/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Now what happens if we model both populations together?&lt;/p&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;popAll &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; data.table&lt;span class=&quot;token operator&quot;&gt;::&lt;/span&gt;rbindlist&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;list&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;popA&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; popB&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
mAll &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; glm&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;success &lt;span class=&quot;token operator&quot;&gt;~&lt;/span&gt; ifasting &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; exercise&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; data&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;popAll&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; family&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;binomial&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
summary&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;mAll&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    ## 
    ## Call:
    ## glm(formula = success ~ ifasting + exercise, family = binomial, 
    ##     data = popAll)
    ## 
    ## Coefficients:
    ##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
    ## (Intercept)  -4.0380     0.6062  -6.661 2.72e-11 ***
    ## ifasting      4.7311     0.5937   7.969 1.60e-15 ***
    ## exercise     -0.1466     0.1713  -0.856    0.392    
    ## ---
    ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 1108.79  on 799  degrees of freedom
    ## Residual deviance:  807.36  on 797  degrees of freedom
    ## AIC: 813.36
    ## 
    ## Number of Fisher Scoring iterations: 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now exercise has a negative coefficient, making it appear that
intermittent fasting and exercise together has &lt;em&gt;worse&lt;/em&gt; outcomes than
intermittent fasting alone!&lt;/p&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;testdata&lt;span class=&quot;token operator&quot;&gt;$&lt;/span&gt;predictAll &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; predict&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;mAll&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; newdata&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;testdata&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; type&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;response&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
testdata&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    ##   ifasting exercise    predictA    predictB predictAll
    ## 1        0        0 0.008988764 0.006688963 0.01732739
    ## 2        0        1 0.020000000 0.010000000 0.01500000
    ## 3        1        0 0.800000000 0.400000000 0.66666667
    ## 4        1        1 0.900000000 0.500000000 0.63333333
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is an example of how Simpson’s paradox might manifest itself in a
logistic regression model, and it’s due to the unmodelled confounding
variable, population type. This, plus some bad luck in the relative sizes of the
treatment groups with respect to population type, lead to the above, counterintuitive, results.&lt;/p&gt;
&lt;p&gt;Simpson’s paradox isn’t the only reason for a coefficient with an
unexpected sign; this can also happen when the data is &lt;a href=&quot;https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-complete-or-quasi-complete-separation-in-logistic-regression-and-what-are-some-strategies-to-deal-with-the-issue/&quot;&gt;separated or
quasi-separated&lt;/a&gt;.
In our case, this would happen if any of the treatment groups perfectly
predicted outcome (that is, if any treatment group completely succeeded
or completely failed). Let’s check for that.&lt;/p&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;popAll &lt;span class=&quot;token operator&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt; 
  mutate&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;gp &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; ifelse&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting &lt;span class=&quot;token operator&quot;&gt;&amp;amp;&lt;/span&gt; exercise&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&#39;both&#39;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
                     ifelse&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ifasting&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&#39;ifast alone&#39;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&#39;exercise alone&#39;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt;
  group_by&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;gp&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt;
  summarize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;success_rate &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; mean&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;success&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    ##               gp success_rate
    ## 1           both    0.6333333
    ## 2 exercise alone    0.0150000
    ## 3    ifast alone    0.6666667
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So separation isn’t the problem; the unmodelled confounding variable is.&lt;/p&gt;
&lt;h2 id=&quot;resolving-the-issue&quot; tabindex=&quot;-1&quot;&gt;Resolving the issue &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-07-16-simpsons-paradox-as-logistic-regression/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When faced with a counterintuitive result, try to determine if there&#39;s
a factor you&#39;re not taking into account.&lt;/p&gt;
&lt;p&gt;In this example, if the researchers did have access to the subjects’ population type,
then they could control for that in the modelling.&lt;/p&gt;
&lt;pre class=&quot;language-r&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-r&quot;&gt;mAllplus &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; glm&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;success &lt;span class=&quot;token operator&quot;&gt;~&lt;/span&gt; ifasting &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; exercise &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; label&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; data&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;popAll&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; family&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;binomial&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
summary&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;mAllplus&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    ## 
    ## Call:
    ## glm(formula = success ~ ifasting + exercise + label, family = binomial, 
    ##     data = popAll)
    ## 
    ## Coefficients:
    ##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
    ## (Intercept)  -4.1433     0.6156  -6.731 1.69e-11 ***
    ## ifasting      5.5836     0.6154   9.073  &amp;lt; 2e-16 ***
    ## exercise      0.5231     0.2037   2.568   0.0102 *  
    ## labelB       -1.9172     0.2103  -9.118  &amp;lt; 2e-16 ***
    ## ---
    ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 1108.79  on 799  degrees of freedom
    ## Residual deviance:  709.51  on 796  degrees of freedom
    ## AIC: 717.51
    ## 
    ## Number of Fisher Scoring iterations: 7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, both &lt;code&gt;ifasting&lt;/code&gt; and &lt;code&gt;exercise&lt;/code&gt; now have positive (and
significant, to p=0.05) coefficients, indicating that both actions
increase the probability of weight loss, and doing them both increases
it even more.&lt;/p&gt;
&lt;p&gt;The model also correctly identifies that subjects of population type B
have a (significantly) lower success rate than subjects from Population
A.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Algorithmic Art</title>
		<link href="https://ninazumel.com/blog/2024-05-27-algorithmic-art/"/>
		<updated>2024-05-27T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2024-05-27-algorithmic-art/</id>
		<content type="html">&lt;p&gt;...or, what nowadays one might call &amp;quot;non-AI generative art.&amp;quot; Over this Memorial Day weekend, I played with a few simple algorithms,
inspired by an &lt;a href=&quot;https://codegolf.stackexchange.com/questions/22144/images-with-all-colors&quot;&gt;old Stack Exchange code golf&lt;/a&gt;.
The intention here isn&#39;t to stomp on AI-based generative art, but just to remember the simple pleasures of creating pretty pictures with math.&lt;/p&gt;
&lt;p&gt;The details are all at &lt;a href=&quot;https://github.com/NinaZumel/AlgorithmicArt/tree/main&quot;&gt;the github repository&lt;/a&gt;, but I thought I&#39;d share some examples here.&lt;/p&gt;
&lt;h2 id=&quot;generate-images-with-color-similarity&quot; tabindex=&quot;-1&quot;&gt;Generate Images with Color Similarity &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-05-27-algorithmic-art/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is inspired by &lt;a href=&quot;https://codegolf.stackexchange.com/a/22326&quot;&gt;the code golf submission&lt;/a&gt; from user fejesjoco, though it&#39;s not the same.
I pick a initial pixel and color at random, then place each subsequent color such that it&#39;s nearest to the colors of its filled neighbors.
The &amp;quot;neighbor distance&amp;quot; metric is the minimum distance from all of a pixel&#39;s filled neighbors.&lt;/p&gt;
&lt;p&gt;The original list of colors was the 32768 15-bit colors,
and the idea was to fill a 256x128 image such that every pixel had a unique color. Here&#39;s what that looks like (resized to a square):&lt;/p&gt;
&lt;img alt=&quot;Image generated by color similarity of 15-bit color list from random start&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/allcolors_random_start-8E6V7h94px-400.png&quot; width=&quot;512&quot; height=&quot;512&quot; srcset=&quot;https://ninazumel.com/img/allcolors_random_start-8E6V7h94px-400.png 400w, https://ninazumel.com/img/allcolors_random_start-8E6V7h94px-512.png 512w&quot; sizes=&quot;30vw&quot;&gt;
&lt;p&gt;I can also generate images using another image as the color source. You can see more examples in &lt;a href=&quot;https://github.com/NinaZumel/AlgorithmicArt/blob/main/nearcolors.ipynb&quot;&gt;my example notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This algorithm generates lovely pictures, but it&#39;s fairly slow, since it must visit every point on the canvas multiple times, and do calculations each time.&lt;/p&gt;
&lt;h2 id=&quot;generate-images-with-a-series-of-random-walks&quot; tabindex=&quot;-1&quot;&gt;Generate Images with a Series of Random Walks &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-05-27-algorithmic-art/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Again, the goal here is to fill the image with unique pixel colors. I pick a start point and a start color at random, then sort the colorlist by distance from the starting point.
Then the starting point picks a random direction (up/down/left/right) to go. If the next pixel hasn&#39;t been filled, then we step there, filling it with the next color on the list.
This continues until we reach a dead end (a point where all the neighbors are already filled). Then we pick another random (unfilled) starting point and (unused) starting color, and take another walk. This continues until every color has been used, which also colors every pixel.&lt;/p&gt;
&lt;p&gt;That looks like this (again, resized to a square):&lt;/p&gt;
&lt;img alt=&quot;Image generated by non-crossing random walks and a 15-bit color list&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/random_walk_15bit_random_start-1qXtqIQ72O-400.png&quot; width=&quot;512&quot; height=&quot;512&quot; srcset=&quot;https://ninazumel.com/img/random_walk_15bit_random_start-1qXtqIQ72O-400.png 400w, https://ninazumel.com/img/random_walk_15bit_random_start-1qXtqIQ72O-512.png 512w&quot; sizes=&quot;30vw&quot;&gt;
&lt;p&gt;This one is not quite as pretty (to my mind), but it&#39;s a little faster. With good bookkeeping, you only visit every point once, with no calculation,
other than the color re-sorting at the beginning of a walk. The problem is that (as we will see), a random walk crosses itself quite frequently if left alone, and
we do reach dead ends quite a lot with this procedure. So I wouldn&#39;t call it fast.&lt;/p&gt;
&lt;p&gt;There are more examples in &lt;a href=&quot;https://github.com/NinaZumel/AlgorithmicArt/blob/main/randomwalk.ipynb&quot;&gt;the example notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;random-walk-of-a-bug-in-paint&quot; tabindex=&quot;-1&quot;&gt;Random Walk of a Bug in Paint &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-05-27-algorithmic-art/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Let&#39;s give up on filling every pixel of the image, though we will walk through all the colors on our colorlist. This time, we&#39;ll put a &amp;quot;bug&amp;quot; (a point) on a square plate, and let it randomly walk around (up/down/left/right), with crossings. Each footstep is a different color from the colorlist. This is quite fast, and can produce some pretty pictures.&lt;/p&gt;
&lt;img alt=&quot;Image generated by a random walk and a 15-bit color list&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/bug_doing_random_walk-nyRzVcYp9r-256.png&quot; width=&quot;256&quot; height=&quot;256&quot;&gt;
&lt;p&gt;Here, I started at a random point with a random color from the 15-bit colorlist, and then sorted the colorlist by distance from the starting color. You can also get some interesting effects by walking the colors from a source image with no re-sorting. Here&#39;s what happened when I used this image&lt;/p&gt;
&lt;img alt=&quot;Hawaiian Chieftains greeting the arrival of Western ships.&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/greeting-by-chiefs-1928-02iyDtsw71-210.png&quot; width=&quot;210&quot; height=&quot;148&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;&lt;em&gt;Greeting by Chiefs&lt;/em&gt; (1928). Artist: Arman Manookian&lt;br&gt; 
Source: &lt;a href=&quot;https://www.wikiart.org/en/arman-manookian/greeting-by-chiefs-1928&quot;&gt;WikiArt&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;and traversed the colors from the painting in order, line by line, as my bug wandered its plate:&lt;/p&gt;
&lt;img alt=&quot;Image generated by a random walk and the colors from the painting Greeting by Chiefs&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/chiefswalk-RNhgBlMJIs-256.png&quot; width=&quot;256&quot; height=&quot;256&quot;&gt;
&lt;p&gt;The primary reason I implemented this was to animate the walk, so my function produces (fairly large) animated gifs, in addition to the final image.
Here&#39;s an &lt;a href=&quot;https://vimeo.com/950859184&quot;&gt;animation of the above image being created&lt;/a&gt;. You can
see how often the bug walks over the same area of the plate. This will give you an idea of why filling the image space by random walk takes so long.&lt;/p&gt;
&lt;p&gt;You can see more examples in the &lt;a href=&quot;https://github.com/NinaZumel/AlgorithmicArt/blob/main/randomwalk_animation.ipynb&quot;&gt;example notebook&lt;/a&gt;. There&#39;s also more &lt;a href=&quot;https://github.com/NinaZumel/AlgorithmicArt/blob/main/more_randomwalks.ipynb&quot;&gt;here&lt;/a&gt;, including the gif-to-mp4 conversion code. The mp4s tend to be much smaller than the original gifs.&lt;/p&gt;
&lt;p&gt;And that&#39;s my weekend so far! Hope yours has been fun, too.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Stupid LLM Tricks, Statistics Version</title>
		<link href="https://ninazumel.com/blog/2024-03-01-stupid-llm-tricks/"/>
		<updated>2024-03-01T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2024-03-01-stupid-llm-tricks/</id>
		<content type="html">&lt;p&gt;I recently posted elsewhere about &lt;a href=&quot;https://ninazumel.com/short_thoughts/blog/2024-02-29-more-stupid-llm-tricks/&quot;&gt;something silly ChatGPT did&lt;/a&gt; when we asked it about states&#39; names. Admittedly, that particular question was merely on the level of cocktail party conversation, or a bar bet; but it does point out that you really should never trust anything it says, unless you already know the answer.&lt;/p&gt;
&lt;p&gt;Here&#39;s what happened when we asked it a statistics question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;&lt;br&gt;
What are some good examples of continuous probability distribution functions with negative skew?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A distribution with negative skew looks like below: tail to the left, the bulk of the distribution to the right (generally, the median is to the right of the mean). There are several &lt;a href=&quot;https://www.statology.org/negatively-skewed-distribution-examples/&quot;&gt;real-life distributions with negative skew&lt;/a&gt;, but we were looking for a continuous &amp;quot;closed-form&amp;quot; distribution (with non-negative support, though that part didn&#39;t make it into the question).&lt;/p&gt;
&lt;img alt=&quot;Graph of a negatively skewed (left-tailed) distribution and its mean&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/negative_skew-C6hjFAfnSa-400.png&quot; width=&quot;1280&quot; height=&quot;960&quot; srcset=&quot;https://ninazumel.com/img/negative_skew-C6hjFAfnSa-400.png 400w, https://ninazumel.com/img/negative_skew-C6hjFAfnSa-800.png 800w, https://ninazumel.com/img/negative_skew-C6hjFAfnSa-1200.png 1200w, https://ninazumel.com/img/negative_skew-C6hjFAfnSa-1280.png 1280w&quot; sizes=&quot;100vw&quot;&gt;
&lt;p&gt;Here&#39;s the answer we got:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Skewness in a probability distribution refers to its asymmetry. A negative skew indicates that the left tail of the distribution is longer or fatter than the right tail, with the bulk of the distribution concentrated on the right side. Some examples of continuous probability distribution functions with negative skew include:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Gamma Distribution: Depending on the parameters, the gamma distribution can exhibit negative skewness, especially when the shape parameter is less than 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;F Distribution: The F distribution can have negative skewness, particularly when the degrees of freedom for the numerator are small compared to the denominator.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Exponential Distribution: While the exponential distribution is often positively skewed, it can exhibit negative skewness for certain parameter values.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Log-Normal Distribution: In some cases, the log-normal distribution can be negatively skewed, especially when the underlying normal distribution has a mean less than 1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Chi-Squared Distribution: The chi-squared distribution can have negative skewness for small degrees of freedom.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;These distributions can be useful in modeling various phenomena where negative skewness is observed, such as in certain financial data or in certain types of measurement errors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;None of these are negatively-skewed distributions.&lt;/strong&gt; In particular, the chi-squared distribution always has non-negative skew for any value of parameters (it&#39;s not the only one, just an obvious one).&lt;/p&gt;
&lt;p&gt;The only negatively-skewed distribution we could think of off the top of our head --&lt;a href=&quot;https://en.wikipedia.org/wiki/Beta_distribution&quot;&gt;the Beta distribution&lt;/a&gt; with appropriate parameters -- didn&#39;t even make the list.&lt;/p&gt;
&lt;p&gt;How is this more useful than, say, asking on the Statistics Stack Overflow (or searching to see if someone has already asked)?&lt;/p&gt;
&lt;p&gt;Someone with a certain amount of statistical background can skim this list and realize that at least some of the answers are clearly wrong. They are then reduced to checking the whole list--which they already could have done with a list of distributions and Wikipedia. And they would have found the Beta distribution quicker from Wikipedia, too.&lt;/p&gt;
&lt;p&gt;Someone with less of a statistical background and not a lot of time might just take the answers as correct, and go on to use them (wrongly!), no matter how many disclaimers OpenAI puts on their interface. And therein lies the problem.&lt;/p&gt;
&lt;h2 id=&quot;llms-are-great-for-remembering-things-you-know&quot; tabindex=&quot;-1&quot;&gt;LLMs are Great for Remembering Things You Know &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-03-01-stupid-llm-tricks/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are two uses for which I find ChatGPT incredibly useful: looking up the answers to simple code questions, and as an intelligent thesaurus.&lt;/p&gt;
&lt;p&gt;An example of the first: &lt;strong&gt;&lt;em&gt;How do I parse an RSS feed in Python?&lt;/em&gt;&lt;/strong&gt; ChatGPT gives me the appropriate library (&lt;code&gt;feedparser&lt;/code&gt;) and some example code. I can easily check if the example code does what I want, and with the name of the library, I can look up the actual documentation.&lt;/p&gt;
&lt;p&gt;It used to be easy to find this sort of information via search, but now the results are so poisoned by paywalls and splogs that it&#39;s far simpler, and more pleasant, to ask ChatGPT. Unlike with clicking on random search results, ChatGPT&#39;s answers are usually accurate the first time.&lt;/p&gt;
&lt;p&gt;An example of the second: &lt;strong&gt;&lt;em&gt;What&#39;s the word for a close-up from a painting? It&#39;s used in the context of art or art history.&lt;/em&gt;&lt;/strong&gt; The answer is &lt;em&gt;detail&lt;/em&gt;, as in &amp;quot;A detail from &lt;em&gt;The Last Supper&lt;/em&gt;, showing Judas clutching a bag of coins,&amp;quot; to describe an image of Judas Iscariot cropped from &lt;em&gt;The Last Supper&lt;/em&gt;. This is a word I try to use in appropriate situations (like crediting images on my blog) but I can never remember it.&lt;/p&gt;
&lt;p&gt;ChatGPT tends to be better at these kind of &amp;quot;What&#39;s that word for...?&amp;quot; questions than standard search, or even than my husband, if he happens to be around (he&#39;s not bad at it, just ChatGPT is better).&lt;/p&gt;
&lt;p&gt;Here&#39;s the thing though: &lt;em&gt;I already knew the word,&lt;/em&gt; I just couldn&#39;t remember it. And I already know Python, just not all of its libraries. I can recognize when I&#39;m getting correct information, and, in these two situations, I can get it quickly. That&#39;s great!&lt;/p&gt;
&lt;h2 id=&quot;caveat-user&quot; tabindex=&quot;-1&quot;&gt;&lt;em&gt;Caveat User&lt;/em&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-03-01-stupid-llm-tricks/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;My other big use for LLMs is translation. DeepL and the AI translator/explainer at Tagalog.com are super useful for my hobbies of &lt;a href=&quot;https://exiw.wordpress.com/translations/&quot;&gt;translating Spanish-language short stories&lt;/a&gt; and &lt;a href=&quot;https://multoghost.wordpress.com/darna-episodes/&quot;&gt;Filipino comics&lt;/a&gt;, respectively -- Spanish and Filipino being languages I can at least somewhat read. And DeepL and Google Translate are helpful when trying to get the gist of a document in a language I don&#39;t read.&lt;/p&gt;
&lt;p&gt;But here&#39;s the thing; I&#39;ve seen these translation engines (even DeepL, which is generally quite good) make some egregious mistakes. I also remember trying to use the AI explainer at Tagalog.com to help me disentangle some sentence where I was getting all mixed up with the tenses and object and subjects. I think it got the grammar correct, but, boy, it just outright &lt;em&gt;lied&lt;/em&gt; to me about the meaning of the word &lt;em&gt;bubuwit&lt;/em&gt;, which means &lt;em&gt;mouse&lt;/em&gt;. I ran the sentence through the explainer multiple times, and got told that &lt;em&gt;bubuwit&lt;/em&gt; means, variously, &lt;em&gt;fish&lt;/em&gt;, &lt;em&gt;worm&lt;/em&gt;, &lt;em&gt;lizard&lt;/em&gt; ... I lost confidence, and checked several dictionaries to make sure: yes, &lt;em&gt;bubuwit&lt;/em&gt; means &lt;em&gt;mouse&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The lesson: don&#39;t entirely trust the machine translations of languages you don&#39;t read. Which I think we already knew, Google Translate having been around for a long time.&lt;/p&gt;
&lt;p&gt;Similarly: don&#39;t entirely trust the code snippets from ChatGPT (or Copilot, I imagine) for programming languages that you don&#39;t know. Copilot and ChatGPT won&#39;t instantly make me a competent Rust programmer, any more than DeepL will make me a competent literary translator of Hungarian. This should also be obvious, but with all the genAI hype, I&#39;m not sure it is.&lt;/p&gt;
&lt;p&gt;Because if I can find glaring errors in areas where I have some expertise, why should I be confident that there aren&#39;t glaring errors in areas I know nothing about?&lt;/p&gt;
&lt;h2 id=&quot;information-literacy&quot; tabindex=&quot;-1&quot;&gt;Information Literacy &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-03-01-stupid-llm-tricks/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I have seen people make the argument: yeah, ChatGPT can spit out bullshit, but why should I trust it less than some random dude on the internet? They spit out bullshit, too. Yes, that&#39;s an issue, and it&#39;s called &lt;a href=&quot;https://www.unesco.org/en/ifap/information-literacy&quot;&gt;information literacy&lt;/a&gt;. Information literacy means the ability to assess the information you&#39;re given, via your own personal competencies, and your ability to recognize the legitimacy and reliability of the &lt;strong&gt;sources&lt;/strong&gt; of the information. It also means consulting multiple sources, when possible.&lt;/p&gt;
&lt;p&gt;But how do you do that, with a monolithic information channel that obscures its sources? Not to mention, just mashes them all up and spits out a plausible sounding response that, in some sense, doesn&#39;t even have a source? All you can rely on, then, is your own personal competencies. Again: &lt;em&gt;don&#39;t ask questions you don&#39;t already know the answer to&lt;/em&gt;, or can&#39;t vet from your own knowledge.&lt;/p&gt;
&lt;p&gt;It&#39;s back to my mantra. &lt;em&gt;Avoid magic; value understanding&lt;/em&gt;. Or as Anil Dash might put it: &lt;a href=&quot;https://www.anildash.com/2023/06/08/ai-is-unreasonable/&quot;&gt;&lt;em&gt;Avoid magic; value reason&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Generative AI can be a great tool, but like any tool, you have to use it the right way.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Clarity, Not Magic</title>
		<link href="https://ninazumel.com/blog/2024-02-21-clarity-not-magic/"/>
		<updated>2024-02-21T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2024-02-21-clarity-not-magic/</id>
		<content type="html">&lt;p&gt;After migrating this blog to Eleventy, I made the comment on Micro.blog that Eleventy &amp;quot;is so much easier to wrap my arms around&amp;quot; than Jekyll was. That got me to wondering--why is that?&lt;/p&gt;
&lt;p&gt;After some thought, I came up with a few reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Newbie-friendly design and documentation that makes the process less opaque.&lt;/li&gt;
&lt;li&gt;Starter code that is simple to read, but that still illustrates useful functionality&lt;/li&gt;
&lt;li&gt;A blog deployment process that is manual, but not mystical.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That last point is the crux, for me. After all, the reason I migrated my site was because the old one broke, and &lt;em&gt;I didn&#39;t know how to fix it&lt;/em&gt;. I won&#39;t lie. The way the old site got automatically built and deployed on Github Pages was mysterious to me. I found a Jekyll template I wanted, and followed some directions that seemed to work, and &lt;em&gt;Voila!&lt;/em&gt; To me, setting up that first website was not that far removed from following some arcane ritual.&lt;/p&gt;
&lt;p&gt;And it worked, until it didn&#39;t. I knew that something, somewhere was going out of date and needed to be updated, but I didn&#39;t know what, or how. And so I had to start from scratch.&lt;/p&gt;
&lt;p&gt;And this time, no magic incantations.&lt;/p&gt;
&lt;p&gt;So, this post will expand upon the points above, but it&#39;s really not about my adventures with Eleventy. It&#39;s meant to be a concrete exploration of some general principles about how to design a process; how to teach (or document) a process; and the difference between informed, purposeful action versus blindly following ritual.&lt;/p&gt;
&lt;p&gt;Read on.&lt;/p&gt;
&lt;img alt=&quot;Illustration of the Sorcerer&#39;s Apprentice trying to bring the broom to life. Ferdinand Barth&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/sorcerer-barth-FaSJaIlHz3-400.png&quot; width=&quot;435&quot; height=&quot;480&quot; srcset=&quot;https://ninazumel.com/img/sorcerer-barth-FaSJaIlHz3-400.png 400w, https://ninazumel.com/img/sorcerer-barth-FaSJaIlHz3-435.png 435w&quot; sizes=&quot;20vw&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;Illustration of Goethe&#39;s &lt;em&gt;Der Zauberlehrling&lt;/em&gt; (The Sorcerer&#39;s Apprentice), 1882. Artist: Ferdinand Barth&lt;br&gt; 
Source: &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Tovenaarsleerling_S_Barth.png&quot;&gt;Wikimedia&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;newbie-friendly-non-opaque-design&quot; tabindex=&quot;-1&quot;&gt;Newbie-Friendly, Non-Opaque Design &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-02-21-clarity-not-magic/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The default way of putting a blog on Github Pages involves some mysterious (to me) interaction of Gemfiles and Github Actions and automated processes that are hidden from the user. I try reading the documentation, but it&#39;s written assuming some background context/knowledge that I don&#39;t have. I can make it work--I&#39;ve put up at least three Jekyll blogs, starting from other people&#39;s templates--but I&#39;ve never really understood the workings of the process. And every time I want to put up a blog, I have to look up how to do it.&lt;/p&gt;
&lt;p&gt;The Eleventy &amp;quot;Get Started&amp;quot; page is so newbie-friendly that it even has a link to &lt;a href=&quot;https://www.11ty.dev/docs/terminal-window/&quot;&gt;a page that explains terminal windows&lt;/a&gt;. It walked me through installing Node.js, and the commands that I need to install Eleventy, build a rudimentary website, and serve it (locally). Now, I don&#39;t know anything about Node.js or JavaScript, but by walking through the tutorial and looking at the artifacts produced: the &lt;code&gt;package.json&lt;/code&gt;, the &lt;code&gt;node_modules&lt;/code&gt; directory, and so on -- I gained some intuition about how the process works.&lt;/p&gt;
&lt;p&gt;Because I feel like I understand what&#39;s going on, I can write down the steps in a way that makes sense to me, and &lt;em&gt;repeat it&lt;/em&gt; -- purposefully, not blindly. In other words, there is something about the design of the software &lt;em&gt;and&lt;/em&gt; its starter documentation that elucidates the process, making it easier to follow.&lt;/p&gt;
&lt;p&gt;Incidentally, the &amp;quot;Get Started&amp;quot; page even shows you how to deploy the baby website, via Netlify. It looked pretty easy, and if I were starting brand-new, I probably would have gone that route. But I&#39;m already on Github Pages, so I decided to stick with that. I was a little nervous about this choice, but it turned out quite well---more on that later.&lt;/p&gt;
&lt;p&gt;Most of the Eleventy documentation still goes over my head, but I&#39;m not worried about that, because I had ...&lt;/p&gt;
&lt;h2 id=&quot;a-simple-but-functional-starter-example&quot; tabindex=&quot;-1&quot;&gt;A Simple but Functional Starter Example &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-02-21-clarity-not-magic/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is an extension of the &amp;quot;Newbie-friendly design&amp;quot; principle. The first time I stood up my site, I knew nothing about &amp;quot;website stuff&amp;quot; except some rudimentary HTML. I needed code examples, to help teach me about CSS, and Liquid, and so on. The built-in Github Pages Jekyll themes were &lt;em&gt;too simple&lt;/em&gt; to help me understand how to build the structure of a blog. The theme I ended up using made a nice site, but it was really &lt;em&gt;too complex&lt;/em&gt; to be an effective learning instrument. So it, too, for a long time, was mostly just magic.&lt;/p&gt;
&lt;p&gt;Eleventy&#39;s &lt;a href=&quot;https://github.com/11ty/eleventy-base-blog&quot;&gt;base blog starter template&lt;/a&gt; has enough functionality to be useful, a clear and simple design, good file organization, and low dependencies: HTML/markdown, CSS, Nunjucks, minimal Javascript. By looking at the code, and by peeking into the &lt;code&gt;package.json&lt;/code&gt; and the &lt;code&gt;node_modules&lt;/code&gt; directory that got generated when I installed the template, I could figure out how things work. I can&#39;t build a theme from scratch, but I can figure out how to modify this one. A little at a time.&lt;/p&gt;
&lt;p&gt;It&#39;s not magic, and eventually, I assume, more of the documentation will make sense. If I need it to.&lt;/p&gt;
&lt;p&gt;My new site is still missing a few things I had before, but it&#39;s not bad-looking, if I do say so myself.&lt;/p&gt;
&lt;h2 id=&quot;non-mystical-deployment&quot; tabindex=&quot;-1&quot;&gt;Non-Mystical Deployment &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-02-21-clarity-not-magic/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This was the step that made me nervous. Jekyll is the default build process for Github Pages sites. How was a non-Jekyll build going to work? Would I be able to do it?&lt;/p&gt;
&lt;p&gt;The official answer is Github Actions, and someone did write an Action for deploying Eleventy sites. But guess what: I don&#39;t understand Github Actions. This felt like bringing more magic into the workflow, and if that Action ever broke---I&#39;d be stuck, again.&lt;/p&gt;
&lt;p&gt;So I looked around, and figured out that I could build and deploy the site manually, with the help of &lt;a href=&quot;https://www.npmjs.com/package/gh-pages&quot;&gt;this package&lt;/a&gt;. It&#39;s far less convenient than having the site just rebuild when I push changes to the source, &lt;em&gt;but I know what it&#39;s doing&lt;/em&gt;. And my site isn&#39;t going to change that frequently, anyway. Under the circumstances, a manual process I understand is preferable to an automation that I don&#39;t understand, and don&#39;t really need. Someday, my assessment of the tradeoffs may change; and then my mind will change, too.&lt;/p&gt;
&lt;p&gt;Right now, it works like a charm.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;But Nina, why is a Github Action magic, but a package you found on the internet not magic?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Yeah, I suppose it&#39;s magic, too, but it&#39;s magic that works over &lt;em&gt;Git&lt;/em&gt;, not &lt;em&gt;Github&lt;/em&gt;. I suspect git is somewhat more stable than any given Github feature. Also, I understand git. So I&#39;d call the package &amp;quot;less magic&amp;quot; than an Action. To me, that is; if you are comfortable with Github Actions, then none of this is magic at all. Go crazy.&lt;/p&gt;
&lt;h2 id=&quot;so-what-have-we-learned&quot; tabindex=&quot;-1&quot;&gt;So, What Have We Learned? &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2024-02-21-clarity-not-magic/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Out of all this, I can try to articulate some general principles, in no particular order.&lt;/p&gt;
&lt;p&gt;Avoid magic. If you do X by blindly following a list of steps you don&#39;t understand, you don&#39;t know how to do X. You&#39;re just following a ritual. And rituals can fail.&lt;/p&gt;
&lt;p&gt;By the above definition, magic is relative.&lt;/p&gt;
&lt;p&gt;A well-designed process is complex enough to be useful, but simple and clear enough to be learnable.&lt;/p&gt;
&lt;p&gt;A good teaching example (code, documentation) should also be complex enough to be useful, but simple and clear enough to be learnable.&lt;/p&gt;
&lt;p&gt;Try to be newbie-friendly. If you can write or design for a user who knows absolutely nothing at the start, but will have &lt;em&gt;something&lt;/em&gt; by the end, someone out there will thank you for it.&lt;/p&gt;
&lt;p&gt;Prefer understanding a process (&amp;quot;no magic&amp;quot;) to streamlining it (opaque automations might be magic).&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Blog Migration</title>
		<link href="https://ninazumel.com/blog/2024-02-19-new-blogging-framework/"/>
		<updated>2024-02-19T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2024-02-19-new-blogging-framework/</id>
		<content type="html">&lt;p&gt;My jekyll-generated website (served by Github Pages) failed to build a couple of weeks ago; I don&#39;t know why. I&#39;m not enough of a front-end developer to diagnose the problem. After looking around for an updated template to move to, I eventually decided to give the &lt;a href=&quot;https://www.11ty.dev/&quot;&gt;Eleventy static site generator&lt;/a&gt; a try.&lt;/p&gt;
&lt;p&gt;So this is my new site, based on Eleventy&#39;s &lt;a href=&quot;https://github.com/11ty/eleventy-base-blog&quot;&gt;official starter blog&lt;/a&gt; template. It wasn&#39;t too hard to figure out, and deployed surprisingly easily! Best of all, Eleventy is built to be &lt;a href=&quot;https://www.11ty.dev/blog/stability/&quot;&gt;stable&lt;/a&gt;; which hopefully means I won&#39;t run into that &lt;em&gt;my website won&#39;t build, and I don&#39;t know why!&lt;/em&gt; problem again.&lt;label for=&quot;mn1&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;
&lt;input type=&quot;checkbox&quot; id=&quot;mn1&quot; class=&quot;margin-toggle&quot;&gt;
&lt;span class=&quot;marginnote&quot;&gt;It&#39;s also designed to &lt;a href=&quot;https://www.11ty.dev/docs/performance/&quot;&gt;build really fast&lt;/a&gt;, but that matters less to me.&lt;/span&gt; Hurray!&lt;/p&gt;
&lt;p&gt;A better developer than I am probably could have migrated the old jekyll site completely, but I didn&#39;t manage to do that. I&#39;ve only carried a few blog posts and pages over for the moment. I&#39;ll put a few more of the old posts back, and my publications page; eventually I&#39;ll figure out how to get my bookshelf and my talks pages back up, too. But at least there&#39;s something here, for now.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Fun With Chat GPT</title>
		<link href="https://ninazumel.com/blog/2024-01-29-fun-with-chatgpt/"/>
		<updated>2024-01-29T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2024-01-29-fun-with-chatgpt/</id>
		<content type="html">&lt;p&gt;So, my husband typed this into ChatGPT 3.5:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What are some English words that are 4 letters long, start with f, and end with k?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;According to ChatGPT, there was only one word that fit that criterion: &lt;em&gt;fork&lt;/em&gt;.  Upon prodding, it admitted that there were maybe a few more:&lt;/p&gt;
&lt;img alt=&quot;Screenshot from ChatGPT 3.5&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/chatgpt_fk-DwBxoayKTh-400.png&quot; width=&quot;3182&quot; height=&quot;2287&quot; srcset=&quot;https://ninazumel.com/img/chatgpt_fk-DwBxoayKTh-400.png 400w, https://ninazumel.com/img/chatgpt_fk-DwBxoayKTh-800.png 800w, https://ninazumel.com/img/chatgpt_fk-DwBxoayKTh-1200.png 1200w, https://ninazumel.com/img/chatgpt_fk-DwBxoayKTh-3182.png 3182w&quot; sizes=&quot;100vw&quot;&gt;
&lt;p&gt;Typing a similar query into DuckDuckGo (&lt;em&gt;4 letter word starting with f ending with k&lt;/em&gt;) produced a page full of links to various crossword puzzle and Scrabble dictionaries. The first link gave me seven words. A little further down, I found a link that gave me thirteen words: faik, fank, feck, filk, fink, firk, fisk, flak, folk, fork, funk, fusk, and of course, &amp;quot;the F-word.&amp;quot;&lt;/p&gt;
&lt;p&gt;I&#39;m sure that ChatGPT has filters to eliminate profanity, and admittedly, not all the above words are &amp;quot;common,&amp;quot; but, come on -- &lt;em&gt;folk&lt;/em&gt;? &lt;em&gt;funk&lt;/em&gt;? &lt;em&gt;flak&lt;/em&gt;??  I particularly like &lt;em&gt;feck&lt;/em&gt;, though I never hear of anyone being &amp;quot;full of feck,&amp;quot; only &amp;quot;feckless&amp;quot; ...&lt;/p&gt;
&lt;p&gt;I&#39;m sure GPT-4 does better, but I don&#39;t care to pay to find out.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>The Sphering Transform for Detecting Distribution Drift</title>
		<link href="https://ninazumel.com/blog/2023-08-21-the-sphering-transform/"/>
		<updated>2023-08-21T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2023-08-21-the-sphering-transform/</id>
		<content type="html">&lt;p&gt;I have a new blog post up on the Win Vector blog: &lt;a href=&quot;https://win-vector.com/2023/08/20/detecting-data-differences-using-the-sphering-transform/&quot;&gt;Detecting Data Differences Using the Sphering Transform&lt;/a&gt;.&lt;/p&gt;
&lt;img alt=&quot;Distributions of sphered norms for two data sets with different distributions&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/new_raw_comp_xformed-s2pbz_kSJE-400.png&quot; width=&quot;609&quot; height=&quot;470&quot; srcset=&quot;https://ninazumel.com/img/new_raw_comp_xformed-s2pbz_kSJE-400.png 400w, https://ninazumel.com/img/new_raw_comp_xformed-s2pbz_kSJE-609.png 609w&quot; sizes=&quot;50vw&quot;&gt;
&lt;p&gt;In this post, I discuss how to convert the problem of detecting changes in multivariate distributions to the simpler problem of detecting changes in univariate distributions. Please check it out.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Pandas-Polars Rosetta Stone</title>
		<link href="https://ninazumel.com/blog/2023-04-10-pandas-polars-rosetta-stone/"/>
		<updated>2023-04-10T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2023-04-10-pandas-polars-rosetta-stone/</id>
		<content type="html">&lt;p&gt;Just created a &lt;a href=&quot;https://github.com/WinVector/Examples/blob/main/pandas_polars_rosettastone/rosetta.ipynb&quot;&gt;handy little Rosetta stone&lt;/a&gt; of common data operations using both the &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;polars&lt;/code&gt; libraries. Maybe you&#39;ll find it useful, too.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>The What, Why, and How of AB Testing</title>
		<link href="https://ninazumel.com/blog/2023-04-06-what-why-how-abtesting/"/>
		<updated>2023-04-07T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2023-04-06-what-why-how-abtesting/</id>
		<content type="html">&lt;p&gt;&lt;em&gt;This article is a shortened version of a &lt;a href=&quot;https://www.wallaroo.ai/blog/the-what-why-and-how-of-a/b-testing&quot;&gt;post from the Wallaroo Blog&lt;/a&gt;, originally written by &lt;a href=&quot;https://www.linkedin.com/in/juliobarros/&quot;&gt;Julio Barros&lt;/a&gt; and me. I&#39;m posting the non-Wallaroo section of that article here, with permission, because I think it&#39;s a useful reference for A/B testing---one that I refer to myself. Hopefully, others find it helpful as well.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;how-to-a-b-test-machine-learning-models&quot; tabindex=&quot;-1&quot;&gt;&lt;strong&gt;How to A/B Test Machine Learning Models&lt;/strong&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2023-04-06-what-why-how-abtesting/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;A/B tests are a key tool of business decision-making. In this article, we&#39;ll discuss the what and why of A/B testing, and how an A/B test is designed.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;what-is-an-a-b-test&quot; tabindex=&quot;-1&quot;&gt;What is an A/B Test? &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2023-04-06-what-why-how-abtesting/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;An &lt;a href=&quot;https://en.wikipedia.org/wiki/A/B_testing&quot;&gt;A/B test&lt;/a&gt;, also called a controlled experiment or a randomized control trial, is a statistical method of determining which of a set of variants is the best. A/B tests allow organizations and policy-makers to make smarter, data-driven decisions that are less dependent on guesswork.&lt;/p&gt;
&lt;p&gt;In the simplest version of an A/B test, subjects are randomly assigned to either the control group (group A) or the treatment group (group B). Subjects in the treatment group receive the treatment (such as a new medicine, a special offer, or a new web page design) while the control group proceeds as normal without the treatment. Data is then collected on the outcomes and used to study the effects of the treatment.&lt;/p&gt;
&lt;img alt=&quot;AB testing&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/AB-Testing-_poswn2rZ2-400.png&quot; width=&quot;900&quot; height=&quot;680&quot; srcset=&quot;https://ninazumel.com/img/AB-Testing-_poswn2rZ2-400.png 400w, https://ninazumel.com/img/AB-Testing-_poswn2rZ2-800.png 800w, https://ninazumel.com/img/AB-Testing-_poswn2rZ2-900.png 900w&quot; sizes=&quot;100vw&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;Author: Seobility - License: &lt;a href=&quot;https://www.seobility.net/en/wiki/Creative_Commons_License_BY-SA_4.0&quot;&gt;[CC BY-SA 4.0]&lt;/a&gt;( &quot;Creative Commons License BY-SA 4.0&quot;)&lt;/p&gt;
&lt;h3 id=&quot;a-b-testing-in-history&quot; tabindex=&quot;-1&quot;&gt;&lt;strong&gt;A/B Testing in History&lt;/strong&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2023-04-06-what-why-how-abtesting/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This idea has been around for a long time. Historically, farmers have divided their fields into sections to test whether various treatments can improve their crop yield. Something like an A/B nutrition test even appears in the Old Testament!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Please test your servants for ten days. Let us be given vegetables to eat and water to drink. You can then compare our appearance with the appearance of the young men who eat the royal rations…&amp;quot; (Daniel 1:12-13)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In 1747, Dr. James Lind conducted one of the earliest clinical trials, testing the &lt;a href=&quot;https://en.wikipedia.org/wiki/James_Lind#Prevention_and_cure_of_scurvy&quot;&gt;efficacy of citrus fruit for curing scurvy&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;a-b-testing-today&quot; tabindex=&quot;-1&quot;&gt;&lt;strong&gt;A/B Testing Today&lt;/strong&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2023-04-06-what-why-how-abtesting/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Today, A/B tests are an important business tool, used to make decisions in areas like product pricing, website design, marketing campaign design, and brand messaging. A/B testing lets organizations quickly experiment and iterate in order to continually improve their business.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In data science&lt;/strong&gt;, A/B tests can also be used to choose between two models in production, by measuring which model performs better in the real world. In this formulation, the control is often an existing model that is currently in production, sometimes called the &lt;strong&gt;&lt;em&gt;champion&lt;/em&gt;&lt;/strong&gt;. The treatment is a new model being considered to replace the old one. This new model is sometimes called the &lt;strong&gt;&lt;em&gt;challenger&lt;/em&gt;&lt;/strong&gt;. In our discussion, we&#39;ll use the terms &lt;em&gt;champion&lt;/em&gt; and &lt;em&gt;challenger&lt;/em&gt;, rather than &lt;em&gt;control&lt;/em&gt; and &lt;em&gt;treatment&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Keep in mind that in machine learning, the terms &lt;em&gt;experiments&lt;/em&gt; and &lt;em&gt;trials&lt;/em&gt; also often refer to the process of finding a training configuration that works best for the problem at hand (this is sometimes called hyperparameter optimization). In this article, we will use the term &lt;em&gt;experiment&lt;/em&gt; to refer to the use of A/B tests to compare the performance of different models in production.&lt;/p&gt;
&lt;h2 id=&quot;how-to-design-an-a-b-test&quot; tabindex=&quot;-1&quot;&gt;&lt;strong&gt;How to Design an A/B Test&lt;/strong&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2023-04-06-what-why-how-abtesting/&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A/B tests are a useful way to rely less on opinions and intuition and to be more data-driven in decision making, but there are a few principles to keep in mind. The experimenter has to decide on a number of things.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First, decide what you are trying to measure.&lt;/strong&gt; We&#39;ll call this the &lt;em&gt;Overall Evaluation Criterion&lt;/em&gt; or &lt;em&gt;OEC&lt;/em&gt;. This may be different and more business-focused than the loss function used while training the models, but it must be something you can measure. Common examples are revenue, click-thru rate, conversion rate, or process completion rate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Second, decide how much better is &amp;quot;better&amp;quot;.&lt;/strong&gt; You might want to just say &amp;quot;Success is when the challenger is better than the champion,&amp;quot; but that&#39;s actually not a testable question, at least not in the statistical sense. You have to decide &lt;em&gt;how much&lt;/em&gt; better the challenger has to be. Let&#39;s define two quantities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;y0&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;:&lt;/em&gt; The champion&#39;s assumed OEC. Since the champion has been running for a while, we should have a good idea of this value. For example, if we are measuring conversion rate, then we might already know that the champion typically achieves a conversion rate of &lt;em&gt;y0 =&lt;/em&gt; 2%.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;𝞭: the &lt;em&gt;minimum delta effect size&lt;/em&gt; we want to reliably detect. This is how much better the challenger needs to be for us to declare it &amp;quot;the winner.&amp;quot; For example, we may decide to switch to our challenger model if it improves the conversion rate by at least 1% – that is, we want the challenger to have a conversion rate of at least 0.02 * (1.01) = 0.0202. This means 𝞭 = 0.002.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that some sample size calculators (we&#39;ll get to that below) specify minimum effect size as a relative delta; in our example, the relative delta is 0.01 (1%), and the absolute delta is 0.002.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Third, decide how much error you want to tolerate&lt;/strong&gt;. Again, you probably want to say &amp;quot;none,&amp;quot; but that isn&#39;t practical. The less error you can tolerate, the more data you need, and in an online setting, the longer you have to run the test. In the classical statistics formulation, an A/B test has the following parameters to describe the error:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;𝞪:&lt;/strong&gt; the &lt;em&gt;significance&lt;/em&gt;, or &lt;em&gt;false positive rate&lt;/em&gt; that we are willing to tolerate. Ideally, we want 𝞪 as small as possible; in practice, 𝞪 is usually set to 0.05. You can think of this as meaning that if we run an A/B test over and over again, we will incorrectly pick an inferior challenger 5% of the time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;𝞫:&lt;/strong&gt; the &lt;em&gt;power&lt;/em&gt;, or &lt;em&gt;true positive rate&lt;/em&gt; we want to achieve. Ideally, we would like 𝞫 near 1; in practice, 𝞫 is usually set to 0.8. You can think of this as meaning that if we run an A/B test over and over again we will correctly pick a superior challenger 80% of the time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that 𝞪 and 𝞫 are talking about incompatible circumstances (that&#39;s why they don&#39;t add up to 1). The first case assumes the challenger is worse, the other case assumes it&#39;s better; finding out which situation we are in is the whole point of an A/B test.&lt;/p&gt;
&lt;p&gt;There&#39;s one last parameter in an A/B test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;n&lt;/em&gt;:&lt;/strong&gt; the minimum number of examples (per model) we have to examine to make sure our false positive rate 𝞪 and true positive rate 𝞫 thresholds are met. Or as it&#39;s commonly said: &amp;quot;to make sure we achieve statistical significance.&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that n is per model: so if you are routing your customers between A and B with a 50-50 split, you need a total experiment size of 2&lt;em&gt;n customers. If you are routing 90% of your traffic to A and 10% to B, then B has to see at least n customers (and A will then see around 9&lt;/em&gt;n). So a 50-50 split is the most efficient, although you may prefer an unbalanced split for other reasons, like safety or stability.&lt;/p&gt;
&lt;p&gt;To run an A/B test, the experimenter picks 𝞪, 𝞫, and the minimum effect size 𝞭, and then determines &lt;em&gt;n&lt;/em&gt;. We won&#39;t go into the formula for calculating &lt;em&gt;n&lt;/em&gt; here; so-called power calculators or sample-size calculators exist to do that for you. &lt;a href=&quot;https://statsig.com/calculator&quot;&gt;Here&#39;s one for rates, from Statsig&lt;/a&gt;; it defaults to 𝞪 = 0.05, 𝞫 = 0.8, and split ratio of 50-50. Feel free to play around to get a sense of how big sample sizes have to be in different situations.&lt;/p&gt;
&lt;p&gt;Once you&#39;ve run the A/B test long enough to achieve the necessary &lt;em&gt;n&lt;/em&gt;, measure the OEC for each model. If OECchallenger - OECchampion &amp;gt; 𝞭, then the challenger wins! Otherwise, you may wish to stick with the champion model.&lt;/p&gt;
&lt;h3 id=&quot;some-practical-considerations&quot; tabindex=&quot;-1&quot;&gt;&lt;strong&gt;Some Practical Considerations&lt;/strong&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2023-04-06-what-why-how-abtesting/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Splitting your subjects:&lt;/strong&gt; When splitting your subjects up randomly between models, make sure the process is truly random, and think through any interference between the two groups. Do they communicate or influence each other in some way? Does the randomization method cause an unintended bias? Any bias in group assignments can invalidate the results. Also, make sure the assignment is consistent so that each subject always gets the same treatment. For example, a specific customer should not get different prices every time they reload the pricing page.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A/A Tests:&lt;/strong&gt; It can be a good idea to run an A/A test, where both groups are control or treatment groups. This can help surface unintentional biases or errors in the processing and can give a better feeling for how random variations can affect intermediate results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Don&#39;t Peek!:&lt;/strong&gt; Due to human nature, it&#39;s difficult not to peek at the results early and draw conclusions or stop the experiment before the minimum sample size is reached. Resist the temptation. Sometimes the &amp;quot;wrong&amp;quot; model can get lucky for a while. You want to run a test long enough to be confident that the behavior you see is really representative and not just a weird fluke.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The more sensitive a test is, the longer it will take:&lt;/strong&gt; The resolution of an A/B test (how small a delta effect size you can detect) increases as the square of the samples. In other words, if you want to halve the delta effect size you can detect, you have to &lt;em&gt;quadruple&lt;/em&gt; your sample size.&lt;/p&gt;
&lt;h3 id=&quot;extensions-to-a-b-testing&quot; tabindex=&quot;-1&quot;&gt;&lt;strong&gt;Extensions to A/B Testing&lt;/strong&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2023-04-06-what-why-how-abtesting/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Bayesian A/B Tests&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The classical (AKA frequentist) statistical approach to A/B testing that we described above can be a bit unintuitive for some people. In particular, note that the definitions of 𝞪 and 𝞫 posit that we run the A/B test over and over; in actuality, we generally run it only once (for a specific A and B). The Bayesian approach takes the data from a single run as a given, and asks, &amp;quot;What OEC values are consistent with what I&#39;ve observed?&amp;quot;&lt;/p&gt;
&lt;p&gt;The general steps for a Bayesian analysis are roughly:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify prior beliefs about possible values of the OEC for the experiment groups. An example prior might be that conversion rates for both groups are different and both between 0 and 10%.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Define a statistical model using a Bayesian analysis tool (ie. using distributional techniques) and flat, uninformative, or equal priors for each group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Collect data and update the beliefs on possible values for the OEC parameters as you go. The distributions of possible OEC parameters start out encompassing a wide range of possible values, and as the experiment continues the distributions tend to narrow and separate (if there is a difference).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Continue the experiment as long as it seems valuable to refine the estimates of the OEC. From the posterior distributions of the effect sizes, it is possible to estimate the delta effect size.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;img alt=&quot;Posterior distributions of a Bayesian treatment and control test.&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/Fig+2-IqZooIpGrI-400.png&quot; width=&quot;712&quot; height=&quot;505&quot; srcset=&quot;https://ninazumel.com/img/Fig+2-IqZooIpGrI-400.png 400w, https://ninazumel.com/img/Fig+2-IqZooIpGrI-712.png 712w&quot; sizes=&quot;100vw&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;Posterior distributions of a Bayesian treatment/control test. Source: &lt;a href=&quot;https://win-vector.com/2013/05/06/bayesian-and-frequentist-approaches-ask-the-right-question/&quot;&gt;Win-Vector Blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note that a Bayesian approach to A/B testing does not necessarily make the test any shorter; it simply makes quantifying the uncertainties in the experiment more straightforward, and arguably more intuitive. For a worked example of frequentist and Bayesian approaches to treatment/control experiments (in the context of clinical trials), see this &lt;a href=&quot;https://win-vector.com/2013/05/06/bayesian-and-frequentist-approaches-ask-the-right-question/&quot;&gt;blog post&lt;/a&gt; from Win Vector LLC.&lt;/p&gt;
&lt;h3 id=&quot;multi-armed-bandits&quot; tabindex=&quot;-1&quot;&gt;&lt;strong&gt;Multi-Armed Bandits&lt;/strong&gt; &lt;a class=&quot;header-anchor&quot; href=&quot;https://ninazumel.com/blog/2023-04-06-what-why-how-abtesting/&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;If you want to minimize the waiting until the end of an experiment before taking action, consider &lt;a href=&quot;https://cxl.com/blog/bandit-tests/&quot;&gt;Multi-Armed Bandit&lt;/a&gt; approaches. Multi-armed bandits dynamically adjust the percentage of new requests that go to each option, based on that option&#39;s past performance. Essentially, the better performing a model is, the more traffic it gets—but some small amount of traffic still goes to poorly performing models, so the experiment can still collect information about them. This balances the trade-off between &lt;em&gt;exploitation&lt;/em&gt; (extracting maximal value by using models that appear to be the best) and &lt;em&gt;exploration&lt;/em&gt; (collecting information about other models, in case they turn out to be better than they currently appear). If a multi-armed bandit experiment is run long enough, it will eventually converge to the best model, if one exists.&lt;/p&gt;
&lt;p&gt;Multi-armed bandit tests can be useful if you can&#39;t run a test long enough to achieve statistical significance; ironically, this situation often occurs when the delta effect size is small, so even if you pick the wrong model, you don&#39;t lose much. In fact, the exploitation-exploration tradeoff means that you potentially gain more value during the experiment than you would have running a standard A/B test.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>New Article on the XI coefficient</title>
		<link href="https://ninazumel.com/blog/2021-12-30-new-article-on-XI-coefficient/"/>
		<updated>2021-12-30T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2021-12-30-new-article-on-XI-coefficient/</id>
		<content type="html">&lt;p&gt;I have a &lt;a href=&quot;https://win-vector.com/2021/12/29/exploring-the-xi-correlation-coefficient/&quot;&gt;new article&lt;/a&gt; up on the Win-Vector blog, about the ξ (&#39;XI&#39;) correlation coeffient that was recently introduced by Professor Sourav Chatterjee in his paper, &lt;a href=&quot;https://arxiv.org/abs/1909.10140&quot;&gt;“A New Coefficient of Correlation”&lt;/a&gt;.&lt;/p&gt;
&lt;img alt=&quot;XICOR coefficient&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/xicor-LotFIJdWtA-400.png&quot; width=&quot;672&quot; height=&quot;480&quot; srcset=&quot;https://ninazumel.com/img/xicor-LotFIJdWtA-400.png 400w, https://ninazumel.com/img/xicor-LotFIJdWtA-672.png 672w&quot; sizes=&quot;100vw&quot;&gt; 
&lt;p&gt;Unlike traditional correlation coefficients, ξ does not assume that the relationship between &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; is linear; in principle, it can be any functional relationship. While this is potentially useful, ξ also has some disadvantages, too.&lt;/p&gt;
&lt;p&gt;In my &lt;a href=&quot;https://win-vector.com/2021/12/29/exploring-the-xi-correlation-coefficient/&quot;&gt;Win-Vector post&lt;/a&gt;, I run some informal experiments to try to get a sense of what different values of ξ might mean.&lt;/p&gt;
&lt;p&gt;Please check it out.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Design, Problem Solving, and Good Taste</title>
		<link href="https://ninazumel.com/blog/2014-11-25-design-problem-solving-and-good-taste/"/>
		<updated>2014-11-25T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2014-11-25-design-problem-solving-and-good-taste/</id>
		<content type="html">&lt;p&gt;I ran across &lt;a href=&quot;http://juretriglav.si/standards-for-graphic-presentation/&quot;&gt;this essay&lt;/a&gt; recently on the role of design standards for scientific data visualization. The author, Jure Triglav, draws his inspiration from the creation and continued use of the NYCTA Graphics Standards, which were instituted in the late 1960s to unify the signage for the New York City subway system.&lt;/p&gt;
&lt;img alt=&quot;New York City subway sign&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/subway-m1XI9ZOr89-400.png&quot; width=&quot;600&quot; height=&quot;378&quot; srcset=&quot;https://ninazumel.com/img/subway-m1XI9ZOr89-400.png 400w, https://ninazumel.com/img/subway-m1XI9ZOr89-600.png 600w&quot; sizes=&quot;60vw&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;Image: &lt;a href=&quot;http://juretriglav.si/standards-for-graphic-presentation/&quot;&gt;A Case for Spaceships&lt;/a&gt; (Jure Triglav)&lt;/p&gt;
&lt;p&gt;As the author puts it, the &lt;a href=&quot;http://thestandardsmanual.com&quot;&gt;Graphics Standards Manual&lt;/a&gt; is &quot;a timeless example of great design elegantly solving a real problem.&quot; Thanks to the unified iconography, a traveler on the New York subway knows exactly what to look for to navigate the subway system, no matter which station they may be in. And the iconography is beautiful, too.&lt;/p&gt;
&lt;img alt=&quot;Photo of Unimark Employees&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/unimark-REZnY1iaRR-400.png&quot; width=&quot;600&quot; height=&quot;337&quot; srcset=&quot;https://ninazumel.com/img/unimark-REZnY1iaRR-400.png 400w, https://ninazumel.com/img/unimark-REZnY1iaRR-600.png 600w&quot; sizes=&quot;60vw&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;
Unimark, the design company that designed the Graphics Standards.&lt;br&gt;Aren&#39;t they a hip, mod looking group? And I&#39;m jealous of those lab coats.&lt;br&gt;Image: &lt;a href=&quot;http://juretriglav.si/standards-for-graphic-presentation/&quot;&gt;A Case for Spaceships&lt;/a&gt; (Jure Triglav)
&lt;/p&gt;
&lt;p&gt;What works to clarify subway travel will work to clarify the morass of graphs and charts that pass for scientific visualization, Triglav argues. And we should start with the work of the Joint Committee on Standards for Graphical Presentation, a group of statisticians, engineers, scientists, and mathematicians who first adopted a set of standards in 1914, revised in 1936, 1938, and 1960. &lt;/p&gt;
&lt;p&gt;I agree with him -- mostly.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://www.jstor.org/stable/2965153 .&quot;&gt;1914&lt;/a&gt; and &lt;a href=&quot;http://hdl.handle.net/2027/wu.89083916932&quot;&gt;1938&lt;/a&gt; reports are online; if you have read William Cleveland&#39;s &lt;em&gt;The Elements of Graphing Data&lt;/em&gt; or Darrell Huff&#39;s &lt;em&gt;How to Lie with Statistics&lt;/em&gt;, you will have seen most of the guidelines before, although I thought the 1938 report in particular was especially clear and eloquent about its guiding principles. It should be noted that the 1938 document emphasizes that the guidelines are only that -- they are not intended as a set of rigid rules.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The committee believes that effective presentation calls for flexibility of treatment rather than for standardization, that each chart should be individually planned with due reference to the special characteristics of the data and the particular use to which the chart is to be put.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;And in fact, while Cleveland and Huff definitely both echo the Joint Committee&#39;s recommendations, they do disagree in places, for example on whether or not a graph of quantities should always include zero. It depends on the use of the chart: exploration vs. presentation. The NYCMTA Graphics Standards address a specific situation; in this case, fixed, rigid standards are appropriate. When we are discussing all of scientific and engineering visualization, the best we can hope for are a set of guiding principles. What the Joint Committee -- and Cleveland, Huff, Tufte, and others -- were really trying to do is to set the standards for good taste.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;People think of taste as a subjective thing, and when it comes to food, fashion, film, art, or literature, that&#39;s true. One person&#39;s great read is another person&#39;s awful junk. I avoid accusing anyone of having &quot;bad taste&quot; when it comes to the subjects above. But in the problem solving disciplines - computer science (coding in particular), engineering, architecture, ergonomics, math, science, and so on - I think that bad taste is a real thing. A real harmful thing. We already use terms like &quot;elegant&quot; or &quot;ugly&quot; when we describe mathematical proofs, or code, or other solutions to various problems. Some &quot;solutions&quot; can be so ugly that they are in fact counterproductive. That&#39;s bad taste. &lt;/p&gt;
&lt;p&gt;Let&#39;s go back to our mass transit example with &lt;a href=&quot;http://www.sarahdoody.com/everyday-ux-bart-ticket-machines-san-francisco/#.VHVcw76ppSU&quot;&gt;this critique&lt;/a&gt; from user experience designer Sarah Doody on the badly designed ticket machines (that were in place at the time of her visit) for BART, San Francisco&#39;s subway/light-rail system. And in her criticism, she didn&#39;t even get to what I&#39;d noticed was the biggest problem for new users: where the purchased ticket came out was &lt;em&gt;nowhere near&lt;/em&gt; either the screen or the slot where you entered money; it came out well below eye level (in the red region of the left image, below). Compare that with the MTA&#39;s ticket machines, where the ticket dispensers are right next to the money insertion slot:&lt;/p&gt;
&lt;img alt=&quot;Old BART ticket machine, compared to MTA ticket machine of the same era&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/tickets-001-je1lhQE0yA-400.png&quot; width=&quot;600&quot; height=&quot;265&quot; srcset=&quot;https://ninazumel.com/img/tickets-001-je1lhQE0yA-400.png 400w, https://ninazumel.com/img/tickets-001-je1lhQE0yA-600.png 600w&quot; sizes=&quot;60vw&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;Left image: &lt;a href=&quot;http://www.nileguide.com/destination/blog/san-francisco-bay-area/2010/08/19/getting-around-the-bay-on-public-transportation/&quot;&gt;Nile Guide&lt;/a&gt;; Right image: &lt;a href=&quot;https://www.flickr.com/photos/revstan/4375747918/&quot;&gt;Rev Stan, Flickr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To my mind, the old BART ticket machine is the equivalent of spaghetti code: technically, it gets the job done, but it&#39;s ugly, incomprehensible, and as Ms. Doody points out, many of the choices that went into its creation are just plain wrong. Its design, such as it was, could be fairly called bad taste. Some people see it the other way:&lt;/p&gt;
&lt;p&gt;
&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;What we are trying to point out is: design is not always just a matter of taste. With enough design principles in mind (such as least astonishment, Liskov substitution, and a few others) you can actually say some design decisions are wrong (and maybe even some day some other design decisions are right). There are very few general principals of software system design, so you really don’t want to ignore the few you have.&lt;/p&gt;&lt;/blockquote&gt;
&lt;div align=&quot;center&quot;&gt;-- John Mount, &lt;a href=&quot;http://www.win-vector.com/blog/2014/09/factors-are-not-first-class-citizens-in-r/&quot;&gt;Factors are not First-class Citizens in R&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;I would argue that design &lt;em&gt;is&lt;/em&gt; a matter of taste. It&#39;s just that, in the problem-solving disciplines, taste is not entirely subjective.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>What&#39;s Wrong with a Low(er)-Stress Job?</title>
		<link href="https://ninazumel.com/blog/2013-01-05-whats-wrong-with-a-lower-stress-job/"/>
		<updated>2013-01-05T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2013-01-05-whats-wrong-with-a-lower-stress-job/</id>
		<content type="html">&lt;p&gt;So there&#39;s this article that&#39;s been making the rounds called &lt;a href=&quot;http://www.careercast.com/jobs-rated/10-least-stressful-jobs-2013&quot;&gt;&quot;The 10 Least Stressful Jobs of 2013&quot;&lt;/a&gt;; perhaps you&#39;ve read it. I don&#39;t normally bother with articles like that, but it came to my attention because some of my old graduate-school friends (who are professors) threw a mini-rant on social media over the fact that University Professor is the Number One least stressful job of the year, according to the article. And just now, I tripped over &lt;a href=&quot;http://www.screwydecimal.com/2013/01/5-reasons-being-librarian-is-stressful.html&quot;&gt;a blog post where a librarian takes umbrage over the fact that they also on the list&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Curiosity led me to finally skim the article in question. I won&#39;t lie, I think some of their choices are odd, and the reasoning that led to those choices is rather simplistic. But I&#39;m not going to quibble with the article itself; frankly, I don&#39;t really care (you notice I only said &quot;skim,&quot; not &quot;read&quot;). What I find interesting is that people are actually &lt;em&gt;offended&lt;/em&gt; at being on that list.&lt;/p&gt;
&lt;p&gt;Think about some jobs that are truly high-stress. Firefighter. Police Officer. Emergency Medical Technician. A case worker for Child Protective Services (especially with budgets the way they are now). Pediatric Oncologist. Heck, what about being unemployed? Seems pretty darned stressful to me.&lt;/p&gt;
&lt;p&gt;A lot of high-stress occupations aren&#39;t well compensated, and many aren&#39;t particularly glamorous. I doubt that people in those professions thought to themselves, &quot;Let&#39;s see, what&#39;s the most stressful job I can think of? I&#39;ll go do that.&quot; I think they do those jobs &lt;em&gt;in spite&lt;/em&gt; of the stress, because of innate vocation, or passion, or the desire to make the world a better place, or whatever. If you could magically make the stress of their jobs disappear, I don&#39;t think they&#39;d complain.&lt;/p&gt;
&lt;p&gt;The problem is that &quot;high-stress job&quot; is often used as code for &quot;challenging job for hard-working, driven people&quot; -- which makes &quot;low-stress job&quot; code for &quot;easy job for lazy people.&quot; Really, they aren&#39;t the same thing. Hard work is a virtue; stress is a disease. A disease is something you should try to alleviate, not celebrate.&lt;/p&gt;
&lt;p&gt;Obviously, some jobs (like firefighter and police officer) are innately stressful, because they are dangerous, or because you always work in emergency conditions. And &lt;a href=&quot;http://www.thedailybeast.com/newsweek/2009/02/13/who-says-stress-is-bad-for-you.html&quot;&gt;a little stress is good for you&lt;/a&gt;, in the sense that stress is what pumps up your adrenaline to meet an immediate challenge, or that some level of difficulty in your job is what keeps it interesting, and what keeps you engaged. But if you aren&#39;t throwing yourself in the way of bullets or into towering infernos on a regular basis, perhaps you might think about how to separate the necessary hard work from the unnecessary demands on your energy.&lt;/p&gt;
&lt;p&gt;You know, like planning ahead and pacing yourself so you don&#39;t get caught unawares by deadlines. Saying &quot;no&quot; to more work when you are already overbooked. Trying not to get overbooked in the first place, to the extent you can avoid it. Figuring out how to sidestep tasks or situations or people that cause you stress or sap your energy. Or at least minimize the time you spend on such energy-drains.&lt;/p&gt;
&lt;p&gt;Those things aren&#39;t possible one hundred percent of the time, and they are more possible in some job situations than others. If you are in a job situation where it is possible, why be ashamed of that? Take advantage of it.&lt;/p&gt;
&lt;p&gt;Or to put it another way: &lt;a href=&quot;http://www.stress-management-for-peak-performance.com/work-related-stress.html&quot;&gt;job-stress often comes from having no control over your situation&lt;/a&gt;, or at least the perception of having no control.&lt;/p&gt;
&lt;p&gt;Is that really how you want your career to define you -- as someone controlled by the job, not vice-versa? Or would you rather be someone who works to meet all your goals and make your mark on the world on your own terms?&lt;/p&gt;
&lt;p&gt;I want to think of myself the second way. And if that means that people call my job &quot;low-stress&quot; -- even in a snarky way -- that&#39;s fine. Part of being in control is not caring about the opinions of people who are obviously wrong.&lt;/p&gt;
&lt;p&gt;Think about it.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>On Balance</title>
		<link href="https://ninazumel.com/blog/2012-12-18-on-balance/"/>
		<updated>2012-12-18T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2012-12-18-on-balance/</id>
		<content type="html">&lt;p&gt;One of my favorite cheesy movies is a gem from 1984 called &lt;a href=&quot;http://en.wikipedia.org/wiki/The_Adventures_of_Buckaroo_Banzai_Across_the_8th_Dimension&quot;&gt;&lt;em&gt;The Adventures of Buckaroo Banzai Across the 8th Dimension&lt;/em&gt;&lt;/a&gt;. For those who haven&#39;t seen it, Buckaroo Banzai is a brilliant young neurosurgeon and particle physicist who spends his days conducting cutting-edge research. At night, he and his research colleagues -- all engineers and scientists and doctors -- rock New Jersey as a band called the Hong Kong Cavaliers. In between the brilliant science and the rock-star night life, the Cavaliers find time to save the world from an alien invasion led by none other than John Lithgow.&lt;/p&gt;
&lt;img alt=&quot;John Lithgow&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/Lithgow-iA5s1FULGt-400.png&quot; width=&quot;400&quot; height=&quot;230&quot;&gt;
&lt;p&gt;It&#39;s very eighties: full of funky jackets with padded shoulders and some crazy hair. I can&#39;t recommend it enough.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube-nocookie.com/embed/AbBMzGUlIRw&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;What happens if you want to be Buckaroo Banzai in real life? Here&#39;s what biologist Adam Ruben writes in his article &lt;a href=&quot;http://sciencecareers.sciencemag.org/career_magazine/previous_issues/articles/2012_12_14/caredit.a1200137&quot;&gt;&quot;The Myth of the Well-Rounded Scientist:&quot;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Scientists with outside interests are often regarded with suspicion in the lab; we can be seen as undedicated, unfocused, easily distracted, and so divorced from the scientific frame of mind that we’ll probably end up working in—oh, the shame—industry.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Ruben started his &quot;Batman job,&quot; as he calls it -- stand-up comedy -- in grad school, and it was in grad school where he learned to hide his extracurricular activity.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;One day, my adviser called me into his office. The campus newspaper had just published a little profile of the stand-up-comedy-performing grad student, and my adviser happened to read it. Over the next 10 minutes, I learned that my hobby was an embarrassment to the department, that there was no way I could properly focus on biology, and that every negative lab result I ever produced was a direct result of telling jokes at night.&lt;/p&gt;
&lt;p&gt;For a moment, I even believed it was true. …&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It&#39;s not true, of course. Very few people truly want to spend every waking hour in the lab (or office) and, &lt;a href=&quot;http://www.salon.com/2012/03/14/bring_back_the_40_hour_work_week/&quot;&gt;frankly, no one should&lt;/a&gt;. And if you&#39;re not in the lab or the office, why shouldn&#39;t you be standing onstage somewhere, performing?&lt;/p&gt;
&lt;p&gt;It&#39;s a variation on &lt;a href=&quot;http://www.theatlantic.com/magazine/archive/2012/07/why-women-still-cant-have-it-all/309020/&quot;&gt;the problem that professional women with children have faced for years:&lt;/a&gt; the so-called &quot;work/life balance&quot; issue. I&#39;ve always hated that phrase. It implies that if you have a &quot;life&quot; -- children, family, other creative interests, what have you -- then by definition, your work isn&#39;t part of your life. And so it follows that you aren&#39;t as &quot;dedicated&quot; or &quot;passionate&quot; about your work as you ought to be.&lt;/p&gt;
&lt;p&gt;And that&#39;s nonsense. I, too, have a &quot;Batman job&quot;: I dance. I have rehearsal three to four times a week, and a performance at least once a month -- sometimes more. I love my Batman job. And I love my day job, too. Being a data scientist &lt;em&gt;is&lt;/em&gt; a part of my life, just as much as being a dancer, a wife, a daughter, and a blogger are all parts of my life.&lt;/p&gt;
&lt;p&gt;It&#39;s not &quot;work/life balance&quot; -- it&#39;s &quot;life balance.&quot; For some people, that balance honestly means spending 99.5% of their waking hours eating, breathing, and dreaming about data science or biology or math or marketing strategy. For others, like me, balance means spending 50% of my time (or whatever 40 hours a week -- sometimes a little more, not much less -- comes to) thinking about data science, and 50% of my time thinking about other things.&lt;/p&gt;
&lt;p&gt;It also means that for that 50% of my time that I spend on data science, I concentrate on data science: my clients&#39; projects, my data science writing, or my data science reading. Having other interests is an advantage here. I am forced to be efficient with the hours I spend on my day job, because no matter what (except for a dire emergency), at 7:30 pm I have to get up and go to rehearsal. I can&#39;t make up for lazy days with long nights in front of the computer.&lt;/p&gt;
&lt;p&gt;The break probably does my brain good, too. The mathematician Jacques Hadamard, in his book &lt;em&gt;The Psychology of Invention in the Mathematical Field&lt;/em&gt;, wrote about what he and Henri Poincaré called &quot;incubation&quot;: the process of letting go of conscious thought about a difficult problem and allowing your unconscious mind to work on it instead. Incubation leads to breakthrough. Here, Hadamard quotes Poincaré:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;When, as must often happen, the thinker makes a false start, he slides insensibly into a groove and may not be able to escape at the moment. … Incubation would consist in getting rid of false leads and hampering assumptions so as to approach the problem with an &#39;open mind.&#39;&quot; We can call this the &lt;em&gt;forgetting-hypothesis&lt;/em&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In other words, when you are stuck on something, stop working and literally forget about it; think about something else. What better way to get your mind out of its rut than to go out and dance, or work on your stand-up comedy routine, or play with your kids? My husband, who is a dedicated computer scientist and mathematician, tells of similar experience. He recently started to teach himself to draw, and tries to spend a at least an hour a day working on sketching exercises. As with any exercise or drill, this can sometimes begin to feel tedious. The way my husband tells it, his brain is so desperate to break out of the tedium that it will of its own volition start chewing on whatever difficult problem has presented itself in his work life. By the time he is done with his exercises, he often has a new breakthrough, and can sit at the computer to attack his sticky problem with a fresh mental outlook.&lt;/p&gt;
&lt;p&gt;Being a 50% data scientist instead of a 99% data scientist doesn&#39;t mean I don&#39;t enjoy what I do for a living. It doesn&#39;t mean I&#39;m not good at it. It doesn&#39;t mean that I take my responsibilities to clients and peers any less seriously than a 99 percenter. It does mean that I have chosen to give up certain achievements that likely come only with a eat-breathe-dream lifestyle. I will never win a Nobel or a Fulkerson Prize. I will never be a world-famous thought leader on data science (or anything else). I will never hold a Cabinet position. I will never run a multimillion-dollar corporation, or be a senior executive at such a corporation. I will never be as rich as Bill Gates or Sergey Brin. &lt;/p&gt;
&lt;p&gt;And that&#39;s okay. As long as I have a good professional reputation, as long as my peers and clients respect me and respect my work, I&#39;m satisfied. Just don&#39;t mistake my lack of ambition for certain measures of success as a lack of dedication to my field -- or a lack of competence.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>I Write, Therefore I Think</title>
		<link href="https://ninazumel.com/blog/2012-10-11-i-write-therefore-i-think/"/>
		<updated>2012-10-11T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2012-10-11-i-write-therefore-i-think/</id>
		<content type="html">&lt;p&gt;I came across an interesting article in &lt;em&gt;The Atlantic&lt;/em&gt; a little while back &lt;a href=&quot;http://www.theatlantic.com/magazine/archive/2012/10/the-writing-revolution/309090/&quot;&gt;that discussed the connection between writing and thinking&lt;/a&gt;. New Dorp, a Staten Island high school in a poor and working-class neighborhood, was able to improve student performance when they realized that their students couldn’t write. These underperforming students often could read and could do math. The majority of them were well-behaved, and seemed to want to learn. Yet they couldn&#39;t pass standard proficiency tests, and couldn&#39;t graduate. All because they couldn&#39;t form complex sentences.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The harder they looked, the teachers began to realize, the harder it was to determine whether the students were smart or not because the tools they had to express their thoughts were so limited that such a judgment was nearly impossible.&lt;/p&gt;&lt;/blockquote&gt;
&lt;img alt=&quot;Fountain Pen. Photo by Nina Zumel&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/pen-o2B6gRdgLI-400.png&quot; width=&quot;450&quot; height=&quot;600&quot; srcset=&quot;https://ninazumel.com/img/pen-o2B6gRdgLI-400.png 400w, https://ninazumel.com/img/pen-o2B6gRdgLI-450.png 450w&quot; sizes=&quot;30vw&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;Photo: Nina Zumel&lt;/p&gt;
&lt;p&gt;The solution the teachers devised was to teach analytical writing skills in almost every class. For example, in chemistry classes, students had to describe the properties of chemical substances in sentences that used specific subordinate clause forms, such as a sentence with &lt;em&gt;although&lt;/em&gt;, or with &lt;em&gt;unless&lt;/em&gt;. Students in literature classes had to hold oral discussions using structured sentence forms: &quot;I (dis)agree with your statement because...&quot;, &quot;Can you explain your answer?&quot; They had to write expository essays in history class.&lt;/p&gt;
&lt;p&gt;The result? Graduation rates have risen from 63% to almost 80% since the beginning of the program. Pass rates on proficiency tests rose from 67% to 89% for the English exam, and 64% to to 75% for the History exam. There&#39;s still room for improvement, but the outlook at New Dorp is optimistic.&lt;/p&gt;
&lt;p&gt;Reading the &lt;em&gt;Atlantic&lt;/em&gt; article, I got the suggestion that knowing how to form sentences and how to express themselves strengthened the students&#39; ability not only to write and to pass tests, but also actually strengthened their ability to think. Once you know that there exist words and phrases like &lt;em&gt;because&lt;/em&gt; or &lt;em&gt;for instance&lt;/em&gt; that help expand your sentences, to elaborate on your basic premises, then you will want to start using them, right? It&#39;s like turning on a faucet.&lt;/p&gt;
&lt;p&gt;This apparently causal relationship between writing and thinking is no surprise for anyone who has to write as part of making a living. I find that I often don&#39;t truly understand a technical topic until I sit down to write about it. And some of my best non-technical writing happens when I have no fixed notion of what I&#39;m going to say, just a vague idea that I must write about some topic, or that I must expand on some passing thought I had over a cup of coffee. The end result of the article or blog post surprises even me. Usually pleasantly, even.&lt;/p&gt;
&lt;p&gt;Here&#39;s Joan Didion, from her wonderful essay &lt;a href=&quot;http://people.bridgewater.edu/~atrupe/ENG310/Didion.pdf&quot;&gt;&quot;Why I Write&quot;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Had I been blessed with even limited access to my own mind there would have been no reason to write. I write entirely to find out what I&#39;m thinking, what I&#39;m looking at, what I see and what it means. What I want and what I fear. ... &lt;em&gt;What is going on in these pictures in my mind?&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;And here&#39;s George Orwell, on why jargon is evil (from the essay &lt;a href=&quot;http://iis.berkeley.edu/sites/default/files/Politics_%26_English_language.pdf&quot;&gt;&quot;Politics and the English Language&quot;&lt;/a&gt;):&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A man may take to drink because he feels himself to be a failure, and then fail all the more completely because he drinks. It is rather the same thing that is happening to the English language. It becomes ugly and inaccurate because our thoughts are foolish, but the slovenliness of our language makes it easier for us to have foolish thoughts. The point is that the process is reversible. Modern English, especially written English, is full of bad habits which spread by imitation and which can be avoided if one is willing to take the necessary trouble. If one gets rid of these habits one can think more clearly, and to think clearly is a necessary first step toward political regeneration: so that the fight against bad English is not frivolous and is not the exclusive concern of professional writers.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Orwell is talking about how words like &quot;freedom,&quot; &quot;terrorist,&quot; or &quot;socialist&quot; gradually lose their true meaning after years of overuse and mis-use, until they become content-free. Dialogue or debate that uses such words therefore also becomes content-free, and we all spend a lot of words saying nothing. When we say nothing, we think nothing: a vicious cycle. Places like New Dorp give us hope that words can feed a virtuous cycle, as well. Words have power.&lt;/p&gt;
&lt;p&gt;On the other hand, the New Dorp approach is rather formula-oriented, isn&#39;t it? It&#39;s old-fashioned, redolent of the old classic high school expository essay form. Remember that? State your thesis. Support it in the body of the essay; each body paragraph has a topic sentence and evidential support. Conclude by restating your thesis and re-examining it in the light of your evidence. The format is clear, and when done properly, it&#39;s an efficient way to get a point across. It&#39;s also usually quite dry.&lt;/p&gt;
&lt;p&gt;There are people who think old-fashioned is a good thing; back to the basics, and all that. There are people who don&#39;t. As some civil-rights activist supposedly once said (sorry, I don&#39;t remember who): &quot;I am suspicious of people who are nostalgic.&quot; With reason: the &quot;good old days&quot; were often not so good, for many different groups of people. In our discussion, that would be the disengaged student.&lt;/p&gt;
&lt;p&gt;So the modern teaching approach, apparently, encourages the student to develop self-expression, in the name of keeping them engaged, and fostering creativity. Sentence structure and the craft of organizing your thoughts don&#39;t seem to be primary lesson goals. So the &lt;em&gt;Atlantic&lt;/em&gt; article claims; I&#39;m not a teacher, and I don&#39;t have school-age children, so I don&#39;t really know. But if it&#39;s true, it&#39;s interesting, because some people would claim that the art of creative writing is the antithesis of thinking. Here&#39;s Robert Olen Butler, from his book &lt;em&gt;From Where You Dream: The Process of Writing Fiction&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Please get out of the habit of saying that you&#39;ve got an &lt;em&gt;idea&lt;/em&gt; for a short story. Art does not come from ideas. Art does not come from the mind. Art comes from the place where you dream. Art comes from your unconscious; it comes from the white-hot center of you. ... If you want to think your way into your fiction, if you think you can analyze your way into a work of art, we&#39;re going to be totally at odds philosophically about what art is and where it comes from.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The truth, as always, is probably somewhere in between, but the point is that by Butler&#39;s argument, teaching creative writing is &lt;em&gt;not&lt;/em&gt; teaching children how to think (it might even be teaching them not to think). And isn&#39;t the point of school to teach children how to think? That&#39;s what they keep telling us. It&#39;s interesting to contrast this with some current discussion about teaching math in schools, for example Andrew Hacker&#39;s now-infamous opinion piece from a few months ago, &quot;&lt;a href=&quot;http://www.nytimes.com/2012/07/29/opinion/sunday/is-algebra-necessary.html?_r=1&amp;amp;pagewanted=all&amp;amp;&quot;&gt;Is Algebra Necessary?&lt;/a&gt;&quot; According to Hacker, algebra is not a life skill; most people won&#39;t ever use it again. Putting aside whether that&#39;s true (it&#39;s not), a similar argument, it seems to me, could be made about creative writing. Why is algebra bad for being allegedly &lt;em&gt;impractical&lt;/em&gt;, while analytical writing is bad for being &lt;em&gt;merely&lt;/em&gt; practical?&lt;/p&gt;
&lt;p&gt;I wouldn&#39;t want the pinnacle of my literary achievement to be the high school expository essay, and I do wish I had taken more classes in creative writing during my formal education. The world would be a bland place if it were merely practical. But if writing well begets thinking well, then it transcends the practical: it&#39;s essential. And what else should we be teaching in schools, if not the essential?&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>On Being a Data Scientist</title>
		<link href="https://ninazumel.com/blog/2012-09-19-on-being-a-data-scientist/"/>
		<updated>2012-09-19T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2012-09-19-on-being-a-data-scientist/</id>
		<content type="html">&lt;p&gt;When people ask me what it means to be a data scientist, I used to answer, &quot;it means you don&#39;t have to hold my hand.&quot; By which I meant that as a data scientist (a consulting data scientist), I can handle the data collection, the data cleaning and wrangling, the analysis, and the final presentation of results (both technical and for the business audience) with a minimal amount of assistance from my clients or their people. Not &lt;em&gt;no&lt;/em&gt; assistance, of course, but little enough that I&#39;m not interfering too much with their day-to-day job.&lt;/p&gt;
&lt;p&gt;This used to be a key selling point, because people with all the necessary skills used to be relatively rare. This is less true now; data science is a hot new career track. Training courses and academic tracks are popping up all over the place. So there is the question: what should such courses teach? Or more to the heart of the question -- what does a data scientist do, and what do they need to know?&lt;/p&gt;
&lt;p&gt;Hilary Mason and Chris Wiggins &lt;a href=&quot;http://www.dataists.com/2010/09/a-taxonomy-of-data-science/&quot;&gt;took a crack at answering that a couple of years ago&lt;/a&gt;. They break down data science, the process, into 5 steps: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Obtain&lt;/strong&gt; the data: in their case from Web APIs. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scrub&lt;/strong&gt; the data: Look for missing data, bad data, outlier. Regularize text data (for instance locations: is &quot;CA&quot; California, or Canada? What about &quot;Cal.&quot;, &quot;Ca&quot;, &quot;California&quot;, &quot;San Francisco&quot;, etc..).
    &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Explore.&lt;/strong&gt; And visualize. Here and during the scrub step is where I might start thinking about the best representations of the data, for modeling. Here is where I begin variable selection.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model.&lt;/strong&gt; And evaluate. This is where the statistics and machine learning knowledge comes in.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interpret.&lt;/strong&gt; And disseminate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That&#39;s basically the breakdown most of us would give. Their focus is on web analysis and on data collection over the web; generally my focus has been on clients who &lt;em&gt;have&lt;/em&gt; the data (albeit in some completely cryptic form); it&#39;s our job to wring some insight from it -- somehow. So in addition to the emphasis that Mason and Wiggins place on scripting languages and unix tools, I would also add knowledge of SQL, and a tool like R that can access data directly from the database for analysis. My colleague John Mount would also add that &lt;a href=&quot;http://www.win-vector.com/blog/2012/07/minimal-version-control-lesson-use-it/&quot;&gt;version-control is a must&lt;/a&gt;. As with software engineering, data science is a process where &quot;that one last tweak&quot; to the model or to the data handling can turn out to be a tweak too many...&lt;/p&gt;
&lt;p&gt;Beyond the tools, and the technical details, though -- what would I add?&lt;/p&gt;
&lt;p&gt;I would add that the process is a loop; more than that, it&#39;s loops within loops. Obtain-Scrub-Explore is often one loop. Scrub (Represent)-Explore-Model can be another loop. It always depends.&lt;/p&gt;
&lt;p&gt;I would add that a healthy understanding of the business processes that generate the data is essential -- otherwise you are apt to &quot;discover&quot; things in the data that everyone (that is, your client) already knows, because they are known artifacts of the business process. The insights in data are like degrees of freedom. Don&#39;t eat up your degrees of freedom on known phenomena. If you don&#39;t have that domain knowledge yourself, make sure your client partners you with a contact who does.&lt;/p&gt;
&lt;p&gt;I would add that a solid understanding of statistics fundamentals is essential (and the whole &lt;a href=&quot;http://www.win-vector.com/blog/&quot;&gt;Win-Vector blog &lt;/a&gt;attests to how much time we spend thinking about fundamentals), but stat and machine learning are not the core of the job. The real science, in my opinion -- the part where you form hypotheses, test them, revise them -- comes less in the modeling and more in the scrub and explore steps. Why does this branch of the bank report recoveries where they never reported losses? What is that &quot;profit&quot; column reporting, really? Does gross national product really predict mortgage defaults, or is it just a proxy variable for time (and in the recent economy, time predicts mortgage default rate pretty well)?&lt;/p&gt;
&lt;p&gt;And there is more science after the modeling, during the evaluation phase. Or more prosaically: the debug phase. Why does the model report absolute nonsense on this one subset of the data? Is the error in the modeling? The data handling? The programming? The &quot;modeling&quot; step itself is actually a very small, and relatively straightforward, part of the overall process.&lt;/p&gt;
&lt;p&gt;No one ever wants to hear this. We all come into the job hoping to wield support vector machines or neural nets like Wonder Woman wields her magic lasso: we capture the data, and then wrest the truth out of it, willy-nilly. I wish. I&#39;d love to wear those cool bullet-deflecting bracelets, too.&lt;/p&gt;
&lt;img alt=&quot;Wonder Woman, by Alex Ross&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/newimage6-4T5gU9-Eur-300.png&quot; width=&quot;300&quot; height=&quot;379&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;Art: Alex Ross&lt;/p&gt;
&lt;p&gt;My point here is that answering the original question is more a discussion of process than a checklist of skills to have and technologies to be familiar with.&lt;/p&gt;
&lt;p&gt;What would you add? What do you think a data scientist needs to know?&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>On Writing Technical Articles for the Nonspecialist</title>
		<link href="https://ninazumel.com/blog/2012-09-04-on-writing-technical-articles-for-the-nonspecialist/"/>
		<updated>2012-09-04T00:00:00Z</updated>
		<id>https://ninazumel.com/blog/2012-09-04-on-writing-technical-articles-for-the-nonspecialist/</id>
		<content type="html">&lt;p&gt;I came across a post from Emily Willingham the other day: &lt;a href=&quot;http://www.emilywillinghamphd.com/2012/08/is-phd-required-for-good-science-writing.html&quot;&gt;&quot;Is a PhD required for Good Science Writing?&quot;&lt;/a&gt;. As a science writer with a science PhD, her answer is: is it not required, and it can often be an impediment. I saw a similar sentiment echoed once by Lee Gutkind, the founder and editor of the journal &lt;em&gt;Creative Nonfiction&lt;/em&gt;. I don&#39;t remember exactly what he wrote, but it was something to the effect that scientists are exactly the wrong people to produce literary, accessible writing about matters scientific.&lt;/p&gt;
&lt;img alt=&quot;A pocket watch. Photo by John Mount&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/watch-5zFYr8EKPV-382.png&quot; width=&quot;382&quot; height=&quot;600&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;Photo: John Mount&lt;/p&gt;
&lt;p&gt;I don&#39;t agree with Gutkind&#39;s point, but I can see where it comes from. Academic writing has a reputation for being deliberately obscure and prolix, jargonistic. Very few people read journal papers for fun (well, except me, but I&#39;m weird). On the other hand, a science writer with a PhD has been trained for critical thinking, and should have a nose for bullpucky, even outside their field of expertise. This can come in handy when writing about medical research or controversial new scientific findings. Any scientist -- any person -- is going to hype up their work. It&#39;s the writer&#39;s job to see through that hype.&lt;/p&gt;
&lt;p&gt;I&#39;m not a science writer in the sense that Dr. Willingham is. I write statistics and data science articles (blog posts) for non-statisticians. Generally, the audience that I write for is professionally interested in the topic, but aren&#39;t necessarily experts at it. And as a writer, many of my concerns are the same as those of a popular science writer.&lt;/p&gt;
&lt;p&gt;I want to cut through the bullpucky. I want you, the reader, to come away understanding something you thought you didn&#39;t -- or even couldn&#39;t -- understand. I want you, the analyst or data science practitioner, to understand your tools well enough to innovate, not just use them blindly. And if I&#39;m writing about one of my innovations, I want you to understand it well enough to possibly use it, not just be awed at my supposed brilliance.&lt;/p&gt;
&lt;p&gt;I don&#39;t do these things perfectly; but in the process of trying, and of reading other writers with similar objectives, I&#39;ve figured out a few things.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It&#39;s not a research paper; don&#39;t write it like a research paper.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To the outsider (and sometimes, to the insider), academic writing is a giant exercise in &lt;a href=&quot;http://en.wikipedia.org/wiki/Gamesmanship&quot;&gt;gamesmanship&lt;/a&gt;. The goals are to demonstrate that you are smarter than your peers, and your research is innovative and novel. Those goals impede clarity; after all, if it&#39;s hard to read, it must be difficult material, and you must be one smart cookie.&lt;/p&gt;
&lt;p&gt;When you are writing for the public, or for the practitioner, you usually aren&#39;t writing about something you invented. This frees you from the pressure of appearing novel, and the topic doesn&#39;t have to seem profound. You don&#39;t have to try to look smart, either. By definition, you know something the reader doesn&#39;t -- and if you can explain it so that it seems obvious in hindsight, you actually look that much more clever.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Channel your inner undergrad.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is related to the first point. Your readers are not your research peers, but they aren&#39;t stupid, either. They just have a different set of expertise. They won&#39;t know all the acronyms or the standard symbology, they won&#39;t know all the terminology or all the logic shortcuts (and occasional sloppy misuse of terminology) that people in the field are accustomed to. They are, however, motivated to learn the subject matter. It&#39;s your job to make it all make sense.&lt;/p&gt;
&lt;p&gt;This is where I have an advantage in writing about statistics: my PhD isn&#39;t in statistics. I learned it after I started working, because I had to. I remember a good deal of frustration with the general presentations of standard material given in textbooks. And I can remember even more frustrations with the &quot;insider&quot; style of writing in research papers -- often a lot of hard work for not a lot of reward. The &lt;a href=&quot;http://www.win-vector.com/blog/category/statistics-to-english-translation/&quot;&gt;&lt;em&gt;Statistics to English&lt;/em&gt;&lt;/a&gt; series of blog posts arose from that experience. It doesn&#39;t have to be that hard.&lt;/p&gt;
&lt;p&gt;Explain everything. If you are going to talk about p-values, for example, then define what they are; define significance, or at least point to a readable reference. Don&#39;t expect your readers to know that π means &quot;vector of probabilities&quot; and not &quot;the area of a unit radius circle.&quot; In any technical field, a clear explanation of definitions -- especially terms and symbols that are used differently in different fields, like &quot;entropy&quot; or &quot;likelihood&quot; -- is worth a lot. It may not seem like much, but believe me, your readers will thank you.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If an explanation helps make sense of a concept to &lt;em&gt;you&lt;/em&gt;, it will probably make sense to someone else.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;... and therefore, it will be helpful to someone, if not everyone. A related point to this is that parroting the textbook or research paper explanations is not always helpful, no matter how official or technical it sounds.&lt;/p&gt;
&lt;p&gt;An example: the definition of &lt;em&gt;likelihood&lt;/em&gt; (or likelihood function) from &lt;em&gt;The Cambridge Dictionary of Statistics&lt;/em&gt; is as follows:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The probability of a set of observations given the value of some parameter or set of parameters.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The dictionary goes on to give an example, the likelihood of a random sample of n observations, &lt;em&gt;x1,..x2,...xn&lt;/em&gt; with probability distribution &lt;em&gt;f(x,θ)&lt;/em&gt;:&lt;/p&gt;
&lt;img alt=&quot;Formula for the likelihood of a random sample of n observations&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/newimage-wTCVlVURcn-236.png&quot; width=&quot;236&quot; height=&quot;100&quot;&gt;
&lt;p&gt;This makes perfect sense, from a 50,000 foot point of view, especially if you are in the habit of thinking statistically. But what if you aren&#39;t? Or if you want to actually calculate a specific likelihood for a specific purpose? Like the person who asked a question about likelihood versus probability &lt;a href=&quot;http://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability&quot;&gt;here&lt;/a&gt;. This person received several excellent answers, most of which were elaborations of the Cambridge definition, with variations on how the likelihood formula was written. This seems to have helped the original questioner; but I can tell you that back in the days when I was struggling to learn these concepts, it wouldn&#39;t have helped me. I learn by concrete examples. Like this (borrowed from one of the answers at the link):&lt;/p&gt;
&lt;p&gt;I have an unfair coin, and I think that the probability that it will land heads is &lt;em&gt;p&lt;/em&gt;. I flip the coin n times, and I record whether it lands heads (x_i=1), or whether it lands tails (x_i=0). The likelihood of observing my series of coin flips, given that I have guessed probability &lt;em&gt;p&lt;/em&gt; correctly is:&lt;/p&gt;
&lt;img alt=&quot;likelihood of a series of coin flips&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/newimage5-5vZ-DOQPmQ-400.png&quot; width=&quot;429&quot; height=&quot;100&quot; srcset=&quot;https://ninazumel.com/img/newimage5-5vZ-DOQPmQ-400.png 400w, https://ninazumel.com/img/newimage5-5vZ-DOQPmQ-429.png 429w&quot; sizes=&quot;30vw&quot;&gt;
&lt;p&gt;Here, I&#39;m using the notation L(x|p) to emphasize that the likelihood of the set of observations X is dependent on my &quot;model&quot; -- the estimate for &lt;em&gt;p&lt;/em&gt;. If the likelihood is high, I have guessed &lt;em&gt;p&lt;/em&gt; correctly; if it is low, I need to re-estimate &lt;em&gt;p&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We can generalize this to any set of observations X that lead to a binary outcome, and any model for the probability, P(x).&lt;/p&gt;
&lt;img alt=&quot;likelihood of a binary outcome&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://ninazumel.com/img/newimage3-xL4snWtU74-400.png&quot; width=&quot;600&quot; height=&quot;97&quot; srcset=&quot;https://ninazumel.com/img/newimage3-xL4snWtU74-400.png 400w, https://ninazumel.com/img/newimage3-xL4snWtU74-600.png 600w&quot; sizes=&quot;100vw&quot;&gt;
&lt;p&gt;Again, if the likelihood isn&#39;t high, then we need to adjust our model P(x), probably by tweaking some parameter or parameters θ within the model. This leads us back to the original Cambridge definition.&lt;/p&gt;
&lt;p&gt;My formula is not as general as the Cambridge formula (or the formulas given by the people who answered the question in my link above), and it may still not be clear enough for everyone. Still, it helped me; this is the formula I use to &lt;a href=&quot;http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/&quot;&gt; explain the use of likelihood in deriving logistic regression&lt;/a&gt;. Judging by Win-Vector&#39;s blog stats, it helps other people out there, too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Few things are &quot;obvious.&quot;&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;But after all, the most successful device in mathmanship is to leave out one or two pages of calculations and for them substitute the word &quot;hence,&quot; followed by a colon. This is guaranteed to hold the reader for a couple of days figuring out how you got hither from hence. Even more effective is to use &quot;obviously&quot; instead of &quot;hence,&quot; since no reader is likely to show his ignorance by seeking help in elucidating anything that is obvious. This succeeds not only in frustrating him but also in bringing him down with an inferiority complex, one of the prime &lt;em&gt;desiderata&lt;/em&gt; of the art.&lt;/p&gt;&lt;/blockquote&gt;
&lt;div style=&quot;text-align:center;&quot;&gt;-- Nicolas Vanserg, &quot;Mathmanship&quot; (1958). Originally published in &lt;em&gt;American Scientist&lt;/em&gt;, collected in &lt;a href=&quot;http://www.amazon.com/Stress-Analysis-Strapless-Evening-Gown/dp/0138526087&quot;&gt;&lt;em&gt;A Stress Analysis of a Strapless Evening Gown: Essays for a Scientific Age.&lt;/em&gt;&lt;/a&gt; Sorry, couldn&#39;t find the essay online.&lt;/div&gt;
&lt;p&gt;If it is &quot;obvious&quot; enough to be explained concisely, then explain it. If the calculation or derivation is short, then spell it out. Otherwise, use another phrase like &quot;it happens to be the case&quot;, or &quot;it can be shown,&quot; with references -- or at least cop to the fact that it&#39;s a messy calculation. Some people will complain about how long your posts are; but there will also be an army of students and other novices who will be grateful to you, because they finally &quot;get it.&quot;&lt;/p&gt;
&lt;p&gt;And finally:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;No matter how hard you try, someone will think you stink. Let it go.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Even if you don&#39;t write your article like a research paper, someone out there will read it (or criticize it) as if it were. If you try to define every concept and explain every chain of reasoning, you will get snarky &quot;TLDR&quot; (too long, didn&#39;t read) comments, or comments to the effect that it was a lot of reading for a point that was &quot;obvious.&quot; If you express an opinion, no matter how mildly or with any number of qualifications (for example, my colleague John Mount&#39;s article on &lt;a href=&quot;http://www.win-vector.com/blog/2012/02/why-i-dont-like-dynamic-typing/&quot;&gt;why he dislikes dynamic typing&lt;/a&gt;), you will get sarcastic comments from those who hold the opposite opinion, on why you are just wrong, or why the issues you point out are your fault, not (e.g.) dynamic typing&#39;s fault. You will get people hurling back at you points that you already addressed in the post.&lt;/p&gt;
&lt;p&gt;So fine, some people don&#39;t read (or are too impatient or closed-minded to read); they aren&#39;t your readers. Just let it go and move on.&lt;/p&gt;
&lt;p&gt;Being the concrete-minded person that I am, these points are addressed to my specific situation; but I hope that they are useful to other science writers and technical writers as well. I&#39;m sure that I&#39;ve violated many of these points in my own writing; hopefully the more I keep at it, the better my writing gets.&lt;/p&gt;
</content>
	</entry>
</feed>
