<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>Adjusting Saturated Multivariate Linear Models</title>
		<meta name="description" content="Data Scientist, San Francisco">
		<link rel="alternate" href="/feed/feed.xml" type="application/atom+xml" title="Nina B. Zumel">
		<link rel="alternate" href="/feed/feed.json" type="application/json" title="Nina B. Zumel">
		<script src="https://kit.fontawesome.com/9bbe6f85f9.js" crossorigin="anonymous"></script>
		
		<style>/**
 * okaidia theme for JavaScript, CSS and HTML
 * Loosely based on Monokai textmate theme by http://www.monokai.nl/
 * @author ocodia
 */

code[class*="language-"],
pre[class*="language-"] {
	color: #f8f8f2;
	background: none;
	text-shadow: 0 1px rgba(0, 0, 0, 0.3);
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	font-size: 1em;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
	border-radius: 0.3em;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #272822;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: #8292a2;
}

.token.punctuation {
	color: #f8f8f2;
}

.token.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.constant,
.token.symbol,
.token.deleted {
	color: #f92672;
}

.token.boolean,
.token.number {
	color: #ae81ff;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #a6e22e;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string,
.token.variable {
	color: #f8f8f2;
}

.token.atrule,
.token.attr-value,
.token.function,
.token.class-name {
	color: #e6db74;
}

.token.keyword {
	color: #66d9ef;
}

.token.regex,
.token.important {
	color: #fd971f;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
/*
 * New diff- syntax
 */

pre[class*="language-diff-"] {
	--eleventy-code-padding: 1.25em;
	padding-left: var(--eleventy-code-padding);
	padding-right: var(--eleventy-code-padding);
}
.token.deleted {
	background-color: hsl(0, 51%, 37%);
	color: inherit;
}
.token.inserted {
	background-color: hsl(126, 31%, 39%);
	color: inherit;
}

/* Make the + and - characters unselectable for copy/paste */
.token.prefix.unchanged,
.token.prefix.inserted,
.token.prefix.deleted {
	-webkit-user-select: none;
	user-select: none;
	display: inline-flex;
	align-items: center;
	justify-content: center;
	padding-top: 2px;
	padding-bottom: 2px;
}
.token.prefix.inserted,
.token.prefix.deleted {
	width: var(--eleventy-code-padding);
	background-color: rgba(0,0,0,.2);
}

/* Optional: full-width background color */
.token.inserted:not(.prefix),
.token.deleted:not(.prefix) {
	display: block;
	margin-left: calc(-1 * var(--eleventy-code-padding));
	margin-right: calc(-1 * var(--eleventy-code-padding));
	text-decoration: none; /* override del, ins, mark defaults */
	color: inherit; /* override del, ins, mark defaults */
}
/* minimal implementation; just plain text in the margin, no blockquotes, code, etc */
/* only (unnumbered) margin notes, no sidenotes */
/* stripped down from https://github.com/edwardtufte/tufte-css */

/* need this for correct sidenote numbering; but I'm not using sidenotes */
/*
body {
    counter-reset: sidenote-counter; 
} 
*/

/* since I imagine these will change with different templates */
:root {
    --content-width: 65%;
    --image-padding: 5%;
    --image-max: 60%;  /* total image+padding <= content-width */
    --notewidth: 30%; 
    /* content-width + notewidth <  100%. I think you should leave some wiggle room. */
    --note-right-margin: -45%; 
    /* if this margin is -(notewidth), the note will start just on the right border of the text */
    /* so push it a little to the right (more negative) */
}

article {
	padding-top: 1rem; 
}

/* make room for the notes in the article container */
article > p,
article > pre,
article > ul {
    width: var(--content-width);
}

/* blockquotes should be a little shorter */
article > blockquote {
    width: calc(var(--content-width) * 0.8);
}

/* keep images/captions in the text column, rather than centered in the article container */
article > img {
    margin-left: 0;
    padding-left: var(--image-padding);
    max-width: var(--image-max); 
}

article > p.caption {
	text-align: left;
    padding-left: var(--image-padding);
	font-style: italic;
	font-size: 0.9em;
	color: #9B9C9D;
 }

 /* finally, the margin note */
.marginnote {    
    float: right;
    clear: right;
    width: var(--notewidth);         
    margin-right: var(--note-right-margin);
    margin-top: 0.3rem;
    margin-bottom: 0;
    font-size: 0.8rem;
    line-height: 1.3;
    vertical-align: baseline;
    position: relative;
}


input.margin-toggle {
    display: none;
}


label.margin-toggle {    
    display: none;

}

/* The mobile fallback: */
/* making max-width bigger makes the note fall inline sooner */
@media (max-width: 768px) {  
    body {
        width: 84%;
        padding-left: 8%;
        padding-right: 8%;
    }

    hr,
    article > p,
    article > blockquote,
    article > pre {
        width: 100%;
    }

    label.margin-toggle {
        display: inline;
    }
       
    .marginnote {           
         display: none;        
        }    

      
    .margin-toggle:checked + .marginnote {            
        display: block;            
        float: left;            
        left: 1rem;            
        clear: both;            
        width: 95%;            
        margin: 1rem 2.5%;            
        vertical-align: baseline;            
        position: relative;        
    }
    label {
        cursor: pointer;
    }
}
* { box-sizing: border-box; }
/* Defaults */
:root {
	--font-family: -apple-system, system-ui, sans-serif;
	--font-family-monospace: Consolas, Menlo, Monaco, Andale Mono WT, Andale Mono, Lucida Console, Lucida Sans Typewriter, DejaVu Sans Mono, Bitstream Vera Sans Mono, Liberation Mono, Nimbus Mono L, Courier New, Courier, monospace;
}

/* Theme colors */
:root {
	--color-gray-20: #e0e0e0;
	--color-gray-50: #C0C0C0;
	--color-gray-70: #4D4D4D;
	--color-gray-90: #333;

	--description-color: #9B9C9D;
	--background-color: #fff;

	--text-color: var(--color-gray-70);
	--text-color-link: #32506d;
	--text-color-link-active: #5f2b48;
	--text-color-link-visited: #17050F;

	--syntax-tab-size: 2;
}

@media (prefers-color-scheme: dark) {
	:root {
		--color-gray-20: #e0e0e0;
		--color-gray-50: #C0C0C0;
		--color-gray-70: #dad8d8;
		--color-gray-90: #dad8d8;

		/* --text-color is assigned to --color-gray-_ above.*/
		--test-color: var(--color-gray-20);
		--text-color-link: #1493fb;
		--text-color-link-active: #6969f7;
		--text-color-link-visited: #a6a6f8;

		--background-color: #15202b;
	}
}


/* Global stylesheet */
* {
	box-sizing: border-box;
}

html,
body {
	padding: 0;
	margin: 0 auto;
	font-family: var(--font-family);
	color: var(--text-color);
	background-color: var(--background-color);
}
html {
	overflow-y: scroll;
}
body {
	max-width: 70em; /* a little wider to accomodate margin notes */
}

/* 
https://css-tricks.com/barebones-css-for-fluid-images/ 
Without this, images generated by eleventy-img will always use the largest available size
-- ie, not be responsive.
*/
img {
	max-width: 100%;
	display: block;
	margin-left: auto;
	margin-right: auto;
  }
  img[width] {
	width: auto; /* Defer to max-width */
  }
  img[width][height] {
	height: auto; /* Preserve aspect ratio */
  }
  
  /* Let SVG scale without boundaries */
  img[src$=".svg"] {
	width: 100%;
	height: auto;
	max-width: none;
  }
  

/* https://www.a11yproject.com/posts/how-to-hide-content/ */
.visually-hidden {
	clip: rect(0 0 0 0);
	clip-path: inset(50%);
	height: 1px;
	overflow: hidden;
	position: absolute;
	white-space: nowrap;
	width: 1px;
}

p:last-child {
	margin-bottom: 0;
}
p {
	line-height: 1.5;
	/* font-size: 1.2em; */
}

p.caption {
	text-align: center;
	font-style: italic;
	font-size: 0.9em;
	color: #9B9C9D;
 }

li {
	line-height: 1.5;
	/* font-size: 1.2em; */
}

a[href] {
	color: var(--text-color-link);
}
a[href]:visited {
	color: var(--text-color-link-visited);
}
a[href]:hover,
a[href]:active {
	color: var(--text-color-link-active);
}

main {
	padding: 1rem;
}
main :first-child {
	margin-top: 0;
}

header {
	border-bottom: 1px dashed var(--color-gray-20);
}
header:after {
	content: "";
	display: table;
	clear: both;
}

.links-nextprev {
	list-style: none;
	border-top: 1px dashed var(--color-gray-20);
	padding: 1em 0;
}

table {
	margin: 1em 0;
}
table td,
table th {
	padding-right: 1em;
}

pre,
code {
	font-family: var(--font-family-monospace);
}
pre:not([class*="language-"]) {
	margin: .5em 0;
	line-height: 1.375; /* 22px /16 */
	-moz-tab-size: var(--syntax-tab-size);
	-o-tab-size: var(--syntax-tab-size);
	tab-size: var(--syntax-tab-size);
	-webkit-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
	direction: ltr;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
}
code {
	word-break: break-all;
}

/*
  Custom containters
*/
div.message-box {
	max-width: 50em; 
	margin-left: auto;
	margin-right: auto;
}



/* Header */
header {
	display: flex;
	gap: 1em .5em;
	flex-wrap: wrap;
	align-items: center;
	padding: 1em;
}
.home-link {
	font-size: 1.5em; /* 24px /16 (I think)*/
	font-weight: 700;
	margin-right: 2em;
}
.home-link:link:not(:hover) {
	text-decoration: none;
}

/* Nav */
.nav {
	display: flex;
	padding: 0;
	margin: 0;
	list-style: none;
}
.nav-item {
	display: inline-block;
	margin-right: 1em;
}
.nav-item a[href]:not(:hover) {
	text-decoration: none;
}
.nav a[href][aria-current="page"] {
	text-decoration: underline;
}

/* Posts list */
.postlist {
	list-style: none;
	padding: 0;
	padding-left: 1.5rem;
	max-width: 50em; /* keep it narrower than the new wider body width */
}
.postlist-item {
	display: flex;
	flex-wrap: wrap;
	align-items: baseline;
	counter-increment: start-from -1;
	margin-bottom: 1em;
}
.postlist-item:before {
	display: inline-block;
	pointer-events: none;
	content: "" counter(start-from, decimal-leading-zero) ". ";
	line-height: 100%;
	text-align: right;
	margin-left: -1.5rem;
}
.postlist-date,
.postlist-item:before {
	font-size: 0.8125em; /* 13px /16 */
	color: var(--color-gray-90);
}
.postlist-date {
	word-spacing: -0.5px;
}
.postlist-link {
	font-size: 1.1875em; /* 19px /16 */
	font-weight: 700;
	flex-basis: calc(100% - 1.5rem);
	padding-left: .25em;
	padding-right: .5em;
	text-underline-position: from-font;
	text-underline-offset: 0;
	text-decoration-thickness: 1px;
}
.postlist-item-active .postlist-link {
	font-weight: bold;
}

/* Tags */
.post-tag {
	display: inline-flex;
	align-items: center;
	justify-content: center;
	text-transform: capitalize;
	font-style: italic;
}
.postlist-item > .post-tag {
	align-self: center;
}

/* Tags list */
.post-metadata {
	display: inline-flex;
	flex-wrap: wrap;
	gap: .5em;
	list-style: none;
	padding: 0;
	margin: 0;
}
.post-metadata time {
	margin-right: 1em;
}

.post-metadata li {
	font-size: 0.875em;
}

/* metadata for posts and talks */

.posttalk-metadata {
	list-style: none;
	padding-left: 0;
	color: var(--description-color);
}

.posttalk-metadata li {
	font-size: 1.1em;
}

/* Direct Links / Markdown Headers */
.header-anchor {
	text-decoration: none;
	font-style: normal;
	font-size: 1em;
	margin-left: .1em;
}
a[href].header-anchor,
a[href].header-anchor:visited {
	color: transparent;
}
a[href].header-anchor:focus,
a[href].header-anchor:hover {
	text-decoration: underline;
}
a[href].header-anchor:focus,
:hover > a[href].header-anchor {
	color: #aaa;
}

h2 + .header-anchor {
	font-size: 1.5em;
}

/* Footer */
hr {
    border: none;
    background-color: #DADADB;
    height: 1px;
    width: 100%;
    margin: 0 auto;
}

@media screen and (max-width: 768px) {
    hr {
        width: 100%;
    }
}

.footer-container {
    display: flex;
    justify-content: space-around;
    align-items: center;
    padding: 2rem 0;
    background-color: var(--background-color0);
	;
}

.footer-container p {
    font-size: 1rem;
    color: #9B9C9D;
}

.footer-container .social_icons {
    display: flex;
    justify-content: space-between;
    align-items: center;
    gap: 0 1rem;
}

.footer-container .social_icons a {
    color: #9B9C9D;
    font-size: 1.5rem;
    transition: all 0.5s ease-in-out;
}

.footer-container .social_icons a:hover {
    color: #DADADB;
}</style>
	</head>
	<body>
		<a href="#skip" class="visually-hidden">Skip to main content</a>

		<header>
			<a href="/" class="home-link">Nina B. Zumel</a>
			<nav>
				<h2 class="visually-hidden">Top level navigation menu</h2>
				<ul class="nav">
					<li class="nav-item"><a href="/pages/professional/">Professional</a></li>
					<li class="nav-item"><a href="/pages/personal/">Personal</a></li>
					<li class="nav-item"><a href="/pages/technical-blogging/">Technical Blogging</a></li>
				</ul>
			</nav>
		</header>

		<main id="skip">
			

<h1>Adjusting Saturated Multivariate Linear Models</h1>

<div class="posttalk-metadata">
  <time datetime="2024-08-20">20 August 2024</time>
</div>
<ul class="post-metadata">
	<li><a href="/tags/data-science/" class="post-tag">data science</a>, </li>
	<li><a href="/tags/statistics/" class="post-tag">statistics</a>, </li>
	<li><a href="/tags/linear-models/" class="post-tag">linear models</a>, </li>
	<li><a href="/tags/censoring/" class="post-tag">censoring</a></li>
</ul>


<article>
<p>This is a followup to the Win Vector blog article <a href="https://win-vector.com/2024/08/16/post-hoc-adjustment-for-zero-thresholded-linear-models/"><em>Post-hoc Adjustment for
Zero-Thresholded Linear
Models</em></a>,
in which I showed how to use a one-variable GAM (e.g., a spline) to
adjust a linear model in a problem space where outcomes are strictly
nonnegative. If you haven’t read that article, I suggest you check it
out, first.</p>
<h2 id="some-background" tabindex="-1">Some Background <a class="header-anchor" href="#some-background">#</a></h2>
<p>The original motivation for the work we did here was to help a client
who had built a fairly complex multivariate model to predict expected
count data. Their underlying assumption was that in their domain, a
count of zero is a rare event. To quote my previous article:</p>
<blockquote>
<p>When you don’t expect to see too many zeros in practice, modeling the
process as linear and thresholding negative predictions at zero is not
unreasonable. But the more zeros (saturations) you expect to see, the
less well a linear model will perform.</p>
</blockquote>
<p>They then automated the model training and deployment process, and put
it into production across multiple sites.</p>
<p>Unfortunately, zero-counts are not so rare as they originally believed,
at some of their sites. Because coming up with and deploying a new model
design is not necessarily feasible at this stage (at least not in the
immediate term), they wanted to figure out how to adjust their existing
models to operate even at high zero-count sites. Note that this
adjustment is a <em>one-dimensional</em> process: mapping the output of one
model to a prediction that is closer to the actual outcomes.</p>
<p>To motivate this adjustment process in the previous article, I used an
example of a linear model that was fit to a one-dimensional saturated
process. This was so I could plot the resulting “hockey stick” function,
and show what happens with different model adjustments. But it’s been my
experience that people won’t believe that this procedure will generalize
to multivariate linear models, unless I show them that it works on such
models. So in this article, I’ll apply the same procedures to a linear
model that was fit to two variables, just to prove the point.</p>
<p>I’ll also add an additional adjustment that wasn’t in the original
article, but was suggested by Neal Fultz: adjustment using a Tobit
model. You can see more details of that in a revised version of the
article on github,
<a href="https://github.com/WinVector/Examples/blob/main/linear_regression_w_zeros/lm_adjust_wtobit.md">here</a>.
Tobit adjustments work nearly as well as GAM adjustments, and do have
the (potential) advantage of having a stronger inductive bias, if you
believe that your process is truly linear in its non-saturated regions
<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p>
<p>In the second part of the article, I’ll try fitting a model directly to
the input data, using both GAM and Tobit, since ideally, that’s what
you’d do if you were approaching this modeling task <em>de novo</em>.</p>
<p>For legibility and brevity, I’m going to hide a lot of the code when
generating this article, but you can find the original R Markdown source
code on github as well,
<a href="https://github.com/WinVector/Examples/blob/main/linear_regression_w_zeros/lm2d_adjust.Rmd">here</a>.</p>
<h2 id="example-problem" tabindex="-1">Example Problem <a class="header-anchor" href="#example-problem">#</a></h2>
<p>Here are the characteristics of our example problem:</p>
<ul>
<li>The feature <code>u</code> positively correlates with outcome, while <code>v</code>
negatively correlates.</li>
<li>The outcome <code>y</code> is constrained to be nonnegative; in other words, it
“saturates” at zero.</li>
<li>Both features <code>u</code> and <code>v</code> are bounded between 0 and 1 uniformly.</li>
<li>There is a moderate noise process on top of it.</li>
</ul>
<pre class="language-r" tabindex="0"><code class="language-r">trueprocess <span class="token operator">=</span> <span class="token keyword">function</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span> <span class="token punctuation">{</span>
  u <span class="token operator">=</span> runif<span class="token punctuation">(</span>N<span class="token punctuation">)</span>
  v <span class="token operator">=</span> runif<span class="token punctuation">(</span>N<span class="token punctuation">)</span>
  noise <span class="token operator">=</span> <span class="token number">0.3</span> <span class="token operator">*</span> rnorm<span class="token punctuation">(</span>N<span class="token punctuation">)</span> 
  y <span class="token operator">=</span> pmax<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> u <span class="token operator">-</span> <span class="token number">3</span> <span class="token operator">*</span> v <span class="token operator">+</span> noise<span class="token punctuation">)</span>
  data.frame<span class="token punctuation">(</span>u <span class="token operator">=</span> u<span class="token punctuation">,</span> v <span class="token operator">=</span> v<span class="token punctuation">,</span> y <span class="token operator">=</span> y<span class="token punctuation">)</span>
<span class="token punctuation">}</span></code></pre>
<h2 id="the-data" tabindex="-1">The Data <a class="header-anchor" href="#the-data">#</a></h2>
<p>Let’s generate some training data.</p>
<pre class="language-r" tabindex="0"><code class="language-r">traind <span class="token operator">=</span> trueprocess<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span>
head<span class="token punctuation">(</span>traind<span class="token punctuation">)</span></code></pre>
<pre class="language-text" tabindex="0"><code class="language-text">    ##           u         v         y
    ## 1 0.1634115 0.2125462 0.6570359
    ## 2 0.5249624 0.6152054 0.0000000
    ## 3 0.3051476 0.9153664 0.0000000
    ## 4 0.9277528 0.8064937 1.5395565
    ## 5 0.5715193 0.9990540 0.0000000
    ## 6 0.2602110 0.5206304 0.0000000</code></pre>
<p>Let’s plot a heatmap of the training data. Note that there’s lots of
saturation.</p>
<img alt="Heatmap of the true process" loading="lazy" decoding="async" src="/img/unnamed-chunk-6-1-Urzcld1ZwK-400.png" width="672" height="480" srcset="/img/unnamed-chunk-6-1-Urzcld1ZwK-400.png 400w, /img/unnamed-chunk-6-1-Urzcld1ZwK-672.png 672w" sizes="100vw">
<!--  ![](lm2d_adjust_files/figure-gfm/unnamed-chunk-6-1.png)-->
<p>We’re going to use all the data to train our models, but it’s not easy
to show comparison plots in three dimensions in a way that’s legible. So
we’ll also look at slices of the data, by holding <code>u</code> restricted to a
narrow range around a nominal value. Here’s a slice with <code>u</code> fixed to
near 0.5.</p>
<img alt="Slice of training data around u=0.5" loading="lazy" decoding="async" src="/img/unnamed-chunk-7-1-8SjDrKYvhC-400.png" width="672" height="480" srcset="/img/unnamed-chunk-7-1-8SjDrKYvhC-400.png 400w, /img/unnamed-chunk-7-1-8SjDrKYvhC-672.png 672w" sizes="100vw">
<!-- ![](lm2d_adjust_files/figure-gfm/unnamed-chunk-7-1.png) -->
<p>As you can see, the data is fairly noisy, in addition to being quite
saturated. Now let’s train a linear model on the data, as well as all
our adjusted models.</p>
<h2 id="part-i-adjustments-to-a-linear-model" tabindex="-1">Part I: Adjustments to a Linear Model <a class="header-anchor" href="#part-i-adjustments-to-a-linear-model">#</a></h2>
<h3 id="train-the-initial-model-and-all-the-adjustments" tabindex="-1">Train the initial model, and all the adjustments <a class="header-anchor" href="#train-the-initial-model-and-all-the-adjustments">#</a></h3>
<p>We’ll fit the model, make the naive adjustment of thresholding the data
at zero, then fit all the candidate adjustment models. Note that only
the initial model ever makes reference to the input variables.</p>
<pre class="language-r" tabindex="0"><code class="language-r"><span class="token comment"># train the initial model, </span>
<span class="token comment"># get the initial predictions and thresholded predictions</span>

initial_model <span class="token operator">=</span> lm<span class="token punctuation">(</span>y <span class="token operator">~</span> u <span class="token operator">+</span> v<span class="token punctuation">,</span> data<span class="token operator">=</span>traind<span class="token punctuation">)</span>
traind<span class="token operator">$</span>y_initial<span class="token operator">=</span> predict<span class="token punctuation">(</span>initial_model<span class="token punctuation">,</span> newdata<span class="token operator">=</span>traind<span class="token punctuation">)</span>
traind<span class="token operator">$</span>y_pred0 <span class="token operator">=</span> pmax<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> traind<span class="token operator">$</span>y_initial<span class="token punctuation">)</span>

<span class="token comment"># fit the adjustment models</span>
<span class="token comment"># (function definitions in R markdown document)</span>
scaling_model <span class="token operator">=</span> fit_scaler<span class="token punctuation">(</span>initial_model<span class="token punctuation">,</span> <span class="token string">"y"</span><span class="token punctuation">,</span> traind<span class="token punctuation">)</span>
linadj_model <span class="token operator">=</span> fit_linmod<span class="token punctuation">(</span>initial_model<span class="token punctuation">,</span> <span class="token string">"y"</span><span class="token punctuation">,</span> traind<span class="token punctuation">)</span>
gamadj_model <span class="token operator">=</span> fit_gammod<span class="token punctuation">(</span>initial_model<span class="token punctuation">,</span> <span class="token string">"y"</span><span class="token punctuation">,</span> traind<span class="token punctuation">)</span>
tobitadj_model <span class="token operator">=</span> fit_tobit<span class="token punctuation">(</span>initial_model<span class="token punctuation">,</span> <span class="token string">"y"</span><span class="token punctuation">,</span> traind<span class="token punctuation">)</span>

adjustments <span class="token operator">=</span> list<span class="token punctuation">(</span>scaling_model<span class="token punctuation">,</span> linadj_model<span class="token punctuation">,</span> gamadj_model<span class="token punctuation">,</span> tobitadj_model<span class="token punctuation">)</span>
names<span class="token punctuation">(</span>adjustments<span class="token punctuation">)</span> <span class="token operator">=</span> c<span class="token punctuation">(</span><span class="token string">"y_linscale"</span><span class="token punctuation">,</span> <span class="token string">"y_linadj"</span><span class="token punctuation">,</span> <span class="token string">"y_gamadj"</span><span class="token punctuation">,</span> <span class="token string">"y_tobitadj"</span><span class="token punctuation">)</span>

<span class="token comment"># make the predictions</span>
<span class="token keyword">for</span><span class="token punctuation">(</span>adj <span class="token keyword">in</span> names<span class="token punctuation">(</span>adjustments<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  traind<span class="token punctuation">[</span><span class="token punctuation">[</span>adj<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> do_predict<span class="token punctuation">(</span>adjustments<span class="token punctuation">[</span><span class="token punctuation">[</span>adj<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> newdata<span class="token operator">=</span>traind<span class="token punctuation">)</span>
<span class="token punctuation">}</span></code></pre>
<p>Let’s look at the RMSE and bias of each model.</p>
<table>
<thead>
<tr>
<th style="text-align:left">prediction_type</th>
<th style="text-align:left">description</th>
<th style="text-align:right">RMSE</th>
<th style="text-align:right">bias</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">initial</td>
<td style="text-align:left">initial model</td>
<td style="text-align:right">0.4438901</td>
<td style="text-align:right">0.0000000</td>
</tr>
<tr>
<td style="text-align:left">pred0</td>
<td style="text-align:left">zero-thresholded model</td>
<td style="text-align:right">0.3721101</td>
<td style="text-align:right">0.0842275</td>
</tr>
<tr>
<td style="text-align:left">linscale</td>
<td style="text-align:left">scale-adjusted model</td>
<td style="text-align:right">0.3692881</td>
<td style="text-align:right">0.1196263</td>
</tr>
<tr>
<td style="text-align:left">linadj</td>
<td style="text-align:left">linear-adjusted model</td>
<td style="text-align:right">0.2453422</td>
<td style="text-align:right">0.0298617</td>
</tr>
<tr>
<td style="text-align:left">gamadj</td>
<td style="text-align:left">GAM-adjusted model</td>
<td style="text-align:right">0.2261695</td>
<td style="text-align:right">0.0007868</td>
</tr>
<tr>
<td style="text-align:left">tobitadj</td>
<td style="text-align:left">Tobit-adjusted model</td>
<td style="text-align:right">0.2293762</td>
<td style="text-align:right">-0.0097394</td>
</tr>
</tbody>
</table>
<p><strong><em>Model RMSE and bias on training data</em></strong></p>
<p>As expected (if you’ve read the previous article), the GAM-adjusted
model has the lowest training RMSE, and also lower bias than the other
adjusted models. The Tobit-adjusted model has comparable RMSE, and
slightly more bias.</p>
<p>We can try to visualize what’s happening, by holding <code>u</code> constant at
different values and looking at slices of the prediction surfaces. We’ll
just compare the original linear model, GAM-adjustment, and
Tobit-adjustment.</p>
<img alt="A comparison of model adjustments across various slices of the prediction surface" loading="lazy" decoding="async" src="/img/unnamed-chunk-10-1-epZZgVNig--400.png" width="768" height="672" srcset="/img/unnamed-chunk-10-1-epZZgVNig--400.png 400w, /img/unnamed-chunk-10-1-epZZgVNig--768.png 768w" sizes="100vw">
<!-- ![](lm2d_adjust_files/figure-gfm/unnamed-chunk-10-1.png) -->
<h3 id="test-on-holdout" tabindex="-1">Test on holdout <a class="header-anchor" href="#test-on-holdout">#</a></h3>
<p>Let’s compare the models on holdout data. For this example the holdout
performances are similar to training.</p>
<pre class="language-r" tabindex="0"><code class="language-r">testd <span class="token operator">=</span> trueprocess<span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">)</span>

testd<span class="token operator">$</span>y_initial<span class="token operator">=</span> predict<span class="token punctuation">(</span>initial_model<span class="token punctuation">,</span> newdata<span class="token operator">=</span>testd<span class="token punctuation">)</span>
testd<span class="token operator">$</span>y_pred0 <span class="token operator">=</span> pmax<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> testd<span class="token operator">$</span>y_initial<span class="token punctuation">)</span>
<span class="token keyword">for</span><span class="token punctuation">(</span>adj <span class="token keyword">in</span> names<span class="token punctuation">(</span>adjustments<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  testd<span class="token punctuation">[</span><span class="token punctuation">[</span>adj<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> do_predict<span class="token punctuation">(</span>adjustments<span class="token punctuation">[</span><span class="token punctuation">[</span>adj<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> newdata<span class="token operator">=</span>testd<span class="token punctuation">)</span>
<span class="token punctuation">}</span></code></pre>
<table>
<thead>
<tr>
<th style="text-align:left">prediction_type</th>
<th style="text-align:left">description</th>
<th style="text-align:right">RMSE</th>
<th style="text-align:right">bias</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">initial</td>
<td style="text-align:left">initial model</td>
<td style="text-align:right">0.4504353</td>
<td style="text-align:right">-0.0046703</td>
</tr>
<tr>
<td style="text-align:left">pred0</td>
<td style="text-align:left">zero-thresholded model</td>
<td style="text-align:right">0.3750385</td>
<td style="text-align:right">0.0836604</td>
</tr>
<tr>
<td style="text-align:left">linscale</td>
<td style="text-align:left">scale-adjusted model</td>
<td style="text-align:right">0.3721833</td>
<td style="text-align:right">0.1184737</td>
</tr>
<tr>
<td style="text-align:left">linadj</td>
<td style="text-align:left">linear-adjusted model</td>
<td style="text-align:right">0.2482977</td>
<td style="text-align:right">0.0310791</td>
</tr>
<tr>
<td style="text-align:left">gamadj</td>
<td style="text-align:left">GAM-adjusted model</td>
<td style="text-align:right">0.2285600</td>
<td style="text-align:right">0.0043918</td>
</tr>
<tr>
<td style="text-align:left">tobitadj</td>
<td style="text-align:left">Tobit-adjusted model</td>
<td style="text-align:right">0.2300590</td>
<td style="text-align:right">-0.0064108</td>
</tr>
</tbody>
</table>
<p><strong><em>Model RMSE and bias on holdout data</em></strong></p>
<h2 id="part-ii-fitting-directly-to-the-input-data" tabindex="-1">Part II : Fitting directly to the input data <a class="header-anchor" href="#part-ii-fitting-directly-to-the-input-data">#</a></h2>
<p>Now let’s try both Tobit and GAM directly on the input data. There are,
of course, other methods we could try, like Poisson or Zero-inflated
Poisson regression. But since we’ve been focusing on Tobit and GAM as
adjustments, we’ll just stick to those.</p>
<p>Note that since neither of these models are restricted to nonnegative
predictions, we still have to threshold to zero.</p>
<p>Let’s fit to our training data.</p>
<pre class="language-r" tabindex="0"><code class="language-r">gam_model <span class="token operator">=</span> gam<span class="token punctuation">(</span>y <span class="token operator">~</span> s<span class="token punctuation">(</span>u<span class="token punctuation">)</span> <span class="token operator">+</span> s<span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">,</span> data<span class="token operator">=</span>traind<span class="token punctuation">)</span>
traind<span class="token operator">$</span>y_gam <span class="token operator">=</span> pmax<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> predict<span class="token punctuation">(</span>gam_model<span class="token punctuation">,</span> newdata<span class="token operator">=</span>traind<span class="token punctuation">)</span><span class="token punctuation">)</span>

tobit_model <span class="token operator">=</span> vglm<span class="token punctuation">(</span>y <span class="token operator">~</span> u <span class="token operator">+</span> v<span class="token punctuation">,</span> tobit<span class="token punctuation">(</span>Lower<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> data<span class="token operator">=</span>traind<span class="token punctuation">)</span>
traind<span class="token operator">$</span>y_tobit <span class="token operator">=</span> pmax<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> predict<span class="token punctuation">(</span>tobit_model<span class="token punctuation">,</span> traind<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">,</span> <span class="token string">"mu"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>
<p>We’ll compare the full model fits to the GAM-adjusted and Tobit-adjusted
linear models.</p>
<table>
<thead>
<tr>
<th style="text-align:left">prediction_type</th>
<th style="text-align:left">description</th>
<th style="text-align:right">RMSE</th>
<th style="text-align:right">bias</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">gamadj</td>
<td style="text-align:left">GAM-adjusted model</td>
<td style="text-align:right">0.2261695</td>
<td style="text-align:right">0.0007868</td>
</tr>
<tr>
<td style="text-align:left">tobitadj</td>
<td style="text-align:left">Tobit-adjusted model</td>
<td style="text-align:right">0.2293762</td>
<td style="text-align:right">-0.0097394</td>
</tr>
<tr>
<td style="text-align:left">gam</td>
<td style="text-align:left">Full GAM model</td>
<td style="text-align:right">0.3505055</td>
<td style="text-align:right">0.0668857</td>
</tr>
<tr>
<td style="text-align:left">tobit</td>
<td style="text-align:left">Full Tobit model</td>
<td style="text-align:right">0.2280450</td>
<td style="text-align:right">-0.0099883</td>
</tr>
</tbody>
</table>
<p><strong><em>Model RMSE and bias on training data</em></strong></p>
<p>The full (thresholded) Tobit model does essentially as well as the
Tobit-adjusted linear model, but the thresholded GAM doesn’t do so well.
Since this problem is truly linear, the stronger inductive bias of the
Tobit model serves us well.</p>
<h3 id="test-on-holdout-1" tabindex="-1">Test on holdout <a class="header-anchor" href="#test-on-holdout-1">#</a></h3>
<p>We can also evaluate these models on holdout data.</p>
<table>
<thead>
<tr>
<th style="text-align:left">prediction_type</th>
<th style="text-align:left">description</th>
<th style="text-align:right">RMSE</th>
<th style="text-align:right">bias</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">gamadj</td>
<td style="text-align:left">GAM-adjusted model</td>
<td style="text-align:right">0.2285600</td>
<td style="text-align:right">0.0043918</td>
</tr>
<tr>
<td style="text-align:left">tobitadj</td>
<td style="text-align:left">Tobit-adjusted model</td>
<td style="text-align:right">0.2300590</td>
<td style="text-align:right">-0.0064108</td>
</tr>
<tr>
<td style="text-align:left">gam</td>
<td style="text-align:left">Full GAM model</td>
<td style="text-align:right">0.3574985</td>
<td style="text-align:right">0.0658623</td>
</tr>
<tr>
<td style="text-align:left">tobit</td>
<td style="text-align:left">Full Tobit model</td>
<td style="text-align:right">0.2280260</td>
<td style="text-align:right">-0.0072236</td>
</tr>
</tbody>
</table>
<p><strong><em>Model RMSE and bias on holdout data</em></strong></p>
<p>We get similar results. We can plot some slices to get an idea of what’s
happening.</p>
<img alt="Comparing Linear to thresholded GAM and Tobit models across various slices of the data" loading="lazy" decoding="async" src="/img/unnamed-chunk-16-1-pL2DBI2WxK-400.png" width="768" height="672" srcset="/img/unnamed-chunk-16-1-pL2DBI2WxK-400.png 400w, /img/unnamed-chunk-16-1-pL2DBI2WxK-768.png 768w" sizes="100vw">
<!-- ![](lm2d_adjust_files/figure-gfm/unnamed-chunk-16-1.png) -->
<h2 id="conclusions" tabindex="-1">Conclusions <a class="header-anchor" href="#conclusions">#</a></h2>
<p>For our original problem—post-hoc adjustments of an already established
modeling procedure—a GAM adjustment seems to be the best way to adapt
our modeling to higher-than-anticipated saturation frequency. However,
Tobit adjustment is competitive. If you have the opportunity to design
the modeling procedure <em>de novo</em>, GAM may not be the best option; though
I admit, I didn’t spend any time trying to tune the model. If the
process you are modeling is well-approximated as linear in the
non-saturated region, then a thresholded Tobit model appears to be a
good choice.</p>
<p>Obviously, to fit a full model, one can try many
more methods: Poisson regression, trees, MARS, boosting, random forest,
and so on. A typical task for the data scientist is to try many
plausible methods on the client’s data and pick the one that appears to
be the best practical trade-off for the given client.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Statisticians generally use the word “censored” when talking about
processes that threshold out to a minimum or maximum value (or
both). Often, the assumption is that the measurements are censored
because of limitations of the measurement itself: for example, you
can’t track a subject for more than five years, so the longest
lifetime you can expect to measure is 5, even if the subject lives
for decades longer. Models like Tobit try to predict <em>as if</em> the
data isn’t censored: they can predict a lifetime longer than 5
years, even though those lifetimes can’t be measured.</p>
<p>I am using the word “saturated” to indicate that the outcome being measured
literally cannot go beyond the threshold: counts can never be
negative. A possibly useful analogy is the image from a digital
camera: too much light “blows out” the photo. If you are trying to
predict the intensity of the light that hit the camera sensor using
the image pixels as the outcome data, then the data is censored. If
you are trying to predict <em>the values of the pixels</em>, then the data
is saturated. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

</article>
<ul class="links-nextprev"><li>Previous: <a href="/blog/2024-07-16-simpsons-paradox-as-logistic-regression/">Simpson’s Paradox as a Logistic Regression</a></li><li>Next: <a href="/blog/2024-09-04-sidenotes/">Playing With Sidenotes</a></li>
</ul>

		</main>

		<!-- footer -->
		<hr />

<div class="footer-container">
	<p>&copy; 2024 Nina B. Zumel</p>
	<div class="social_icons">
	    <a
			href="mailto:nzumel@win-vector.com?subject=Comment%20from%20Website"
			aria-label="Email"
			target="_blank"
			rel="noopener noreferrer"
		>
			<i class="fas fa-fw fa-envelope"></i>
		</a>
		<a
			href="https://github.com/ninazumel"
			aria-label="GitHub"
			target="_blank"
			rel="noopener noreferrer"
		>
			<i class="fa-brands fa-github"></i>
		</a>
		<a
			href="https://www.linkedin.com/in/ninazumel/"
			aria-label="LinkedIn"
			target="_blank"
			rel="noopener noreferrer"
		>
			<i class="fa-brands fa-linkedin"></i>
		</a>
		<a
			href="/feed.xml"
			aria-label="RSS"
			target="_blank"
			rel="noopener noreferrer"
		>
			<i class="fa fa-fw fa-rss-square"></i>
		</a>

	</div>
</div>
		<!-- /footer-- >

		<!-- Current page: /blog/2024-08-20-saturated-models/ -->
	</body>
</html>
